{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy\n",
    "#pip install pandas\n",
    "#pip install pandas-profiling\n",
    "#pip install networkx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path)\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n",
    "\n",
    "def plot_graph(g,ds_nodes=[],attribute_nodes=[],feat_nodes=[],lit_nodes=[]):\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=ds_nodes,node_color=\"blue\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=attribute_nodes,node_color=\"green\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=feat_nodes,node_color=\"grey\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=lit_nodes,node_color=\"red\",node_size=900)\n",
    "\n",
    "    nx.draw_networkx_edges(g,pos,width=3)\n",
    "    nx.draw_networkx_labels(g,pos,font_size=8)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data_Sets construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset(datasets,instances=0):\n",
    "    g = nx.Graph()\n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    ds_nodes = []\n",
    "    feat_nodes = []\n",
    "    lit_nodes = []\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        node_id = \"DS_\"+str(datasets.iloc[r][0])\n",
    "        ds_nodes.append(node_id)\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset = features[i]+\"_\"+node_id\n",
    "            g.add_edge(node_id,feature_dataset)\n",
    "            g.add_edge(feature_dataset,row[i])\n",
    "            feat_nodes.append(feature_dataset)\n",
    "            lit_nodes.append(row[i])\n",
    "            \n",
    "    return g,ds_nodes,feat_nodes,lit_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Attribute construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_attribute(datasets,instances=0,graph=None,ds_nodes=None,feat_nodes=None,lit_nodes=None,attr_nodes=None):\n",
    "    if graph == None:\n",
    "        g = nx.Graph()\n",
    "    else:\n",
    "        g = graph\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    if ds_nodes == None:\n",
    "        ds_nodes = []\n",
    "    if feat_nodes == None:\n",
    "        feat_nodes = []\n",
    "    if lit_nodes == None:\n",
    "        lit_nodes = []\n",
    "    if attr_nodes == None:\n",
    "        attr_nodes = []\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        node_dataset_id = \"DS_\"+str(datasets.iloc[r][0])\n",
    "        node_attribute_id = datasets.iloc[r][1] + \"_\"+node_dataset_id\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        attr_nodes.append(node_attribute_id)\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(node_dataset_id,node_attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute = features[i]+\"_\"+str(node_attribute_id)\n",
    "            g.add_edge(node_attribute_id,feature_attribute)\n",
    "            g.add_edge(feature_attribute,row[i])\n",
    "            feat_nodes.append(feature_attribute)\n",
    "            lit_nodes.append(row[i])\n",
    "            \n",
    "    return g,ds_nodes,attr_nodes,feat_nodes,lit_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = read_dataset(\"./openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g,ds_nodes,feat_nodes,lit_nodes=graph_dataset(df_dataset,instances=0);\n",
    "df_attributes = read_dataset(\"./openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g,ds_nodes,attr_nodes,feat_nodes,lit_nodes = graph_attribute(df_attributes,0,g,ds_nodes,feat_nodes,lit_nodes)\n",
    "df_attributes_numeric = read_dataset(\"./openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g,ds_nodes,attr_nodes,feat_nodes,lit_nodes = graph_attribute(df_attributes_numeric,0,g,ds_nodes,feat_nodes,lit_nodes,attr_nodes)\n",
    "#plot_graph(g,ds_nodes,attr_nodes,feat_nodes,lit_nodes);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "10931\n",
      "138612\n",
      "138612\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_nodes))\n",
    "print(len(attr_nodes))\n",
    "print(len(feat_nodes))\n",
    "print(len(lit_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possitive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    possitive_pairs = []\n",
    "    negative_pairs = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[2] == 1:\n",
    "            possitive_pairs.append((row[0],row[1]))\n",
    "        else:\n",
    "            if randrange(1, 10) > 8:\n",
    "                negative_pairs.append((row[0],row[1]))\n",
    "    return possitive_pairs,negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possitive samples: 543 Negative samples (10%): 2166\n"
     ]
    }
   ],
   "source": [
    "df_matching = read_dataset(\"./openml_203ds_datasets_matching.csv\",keep_columns=[\"'dataset1_id'\", \"'dataset2_id'\",\"'matching_topic'\"]);\n",
    "pos,neg = get_samples(df_matching)\n",
    "print(\"Possitive samples: \"+str(len(pos)) + \" Negative samples (10%): \"+str(len(neg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    \"\"\" Returns True is string is a number. \"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tokens(nodes):\n",
    "    #nodes_unique = set(nodes)\n",
    "    nodes_unique = nodes\n",
    "    numerical=[]\n",
    "    nominal=[]\n",
    "    for s in nodes_unique:\n",
    "        if is_number(s):\n",
    "            numerical.append(float(s))\n",
    "        else:\n",
    "            #nominal = nominal +  list(map(lambda a : re.sub(r'[^A-Za-z0-9 \\']+','',a).lower(),(re.split('[-_;|]\\s*',s))))\n",
    "            nominal.append(re.sub(r'[^A-Za-z0-9 \\']+',' ',s).lower())\n",
    "            #nominal = nominal +  s.split(\"|\")\n",
    "    return list(set(numerical)),list(set(nominal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_from_number(n):\n",
    "    #get the embeddings for numerical values that make sense\n",
    "    out = []\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_num,vec_nom = get_tokens(lit_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num: 0.0\n",
      "nom: not enrolled other private 1970 parochial 1970 public school\n",
      "num: 1.0\n",
      "nom: 0 3 6 9 12 15 18 21 24 27 30 33 39\n",
      "num: 2.0\n",
      "nom: 1 2 3 4 6 7 8 10 12 13 14 15 16\n",
      "num: 3.0\n",
      "nom: brickface sky foliage cement window path grass\n",
      "num: 4.0\n",
      "nom: mnist 784\n",
      "num: 5.0\n",
      "nom: icu\n",
      "num: 3.25\n",
      "nom: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
      "num: 6.0\n",
      "nom: 0 1 25 55 65 75 85 95 105 115 125 135 145 155 165 175 185 195 212 237 262 287 325 375 450 500\n",
      "num: 7.0\n",
      "nom: 0 23 38 55 128 150\n",
      "num: 8.0\n",
      "nom: 0 7 25 51 52 81 107 132 165 221 252 253\n",
      "num: 10.0\n",
      "nom: 0 5 9 24 74 75 100 180 182 234 341 348 350 385 400 405 460 465 470 480 500 520 543 584 620 630 640 660 693 694 700 716 720 782 798 800 840 888 924 936 948 1000 1011 1057 1128 1176 1200 1236 1250 1308 1344 1400 1440 1464 1500 1504 1572 1600 1620 1632 1680 1700 1800 1864 1965 1980 1998 2000 2016 2040 2052 2100 2172 2280 2300 2320 2400 2430 2448 2496 2500 2508 2544 2556 2580 2628 2640 2680 2736 2753 2772 2800 2924 3000 3070 3084 3108 3120 3180 3192 3215 3318 3360 3420 3432 3468 3486 3500 3534 3540 3600 3612 3624 3642 3744 3820 3864 3982 4000 4002 4012 4032 4075 4080 4092 4100 4296 4320 4416 4480 4488 4500 4560 4595 4600 4668 4683 4750 4800 4848 4944 5000 5040 5116 5196 5200 5316 5644 5676 5688 5760 5880 5904 5998 6000 6012 6014 6016 6120 6140 6144 6196 6224 6360 6500 6504 6516 6576 6695 6696 6700 6720 6840 6960 6970 6996 7000 7002 7024 7060 7116 7200 7224 7296 7300 7320 7400 7464 7500 7560 7596 7600 7764 7956 8000 8004 8016 8040 8100 8136 8208 8240 8256 8280 8300 8328 8338 8448 8500 8508 8988 9179 9600 9688 9888 15189 99999\n",
      "num: 9.0\n",
      "nom: lowbwt\n",
      "num: 12.0\n",
      "nom: 58 59 60 61 62 63 64 65 66 67 68 69\n",
      "num: 4.1875\n",
      "nom: 0 76\n",
      "num: 7.75\n",
      "nom: 0 7 14 21 28 35 42 49 56 63 84 105 119\n",
      "num: 15.0\n",
      "nom: seuy lvza ctuh\n",
      "num: 16.0\n",
      "nom: gina agnostic\n",
      "num: 17.0\n",
      "nom: huia prop tahora\n",
      "num: 2.5\n",
      "nom: 3130 4673 6725 19721 19793 117887 118131 118205 118295 118331 118347 118363 118372 118398 118424 118453 118467 118474 118478 118504 118612 118638 118643 118667 118704 118736 118762 118870 118960 119006 119095 119184 119221 119695 119772 119784 119788 120134 120302 120518 121069 121620 121916 122032 123611 123689 124136 124145 124487 125407 127957 130364 131999 132725 136398 143398 149353 151277 155173 159679 161100 249618 254395 270488 290919 292795 308574\n",
      "num: 19.0\n",
      "nom: 0 7 37\n",
      "num: 20.0\n",
      "nom: accounting auditing and bookkeeping services advertising agriculture air transportation aircraft and parts apparel and accessories apparel and accessories stores except shoe auto repair services and garages bakery products banking and credit beverage industries blast furnaces steel works 336 rolling mills bowling alleys and billiard and pool parlors canning and preserving fruits vegetables and seafoods carpets rugs and other floor coverings cement concrete gypsum and plaster products coal mining confectionary and related products construction crude petroleum and natural gas extraction dairy prods stores and milk retailing dairy products dressmaking shops drug stores drugs and medicines drugs chemicals and allied products dry goods apparel eating and drinking places educational services electric gas utilities electric light and power electrical goods hardware and plumbing equipment electrical machinery equipment and supplies engineering and architectural services fabricated steel products federal public administration fisheries five and ten cent stores food and related products food stores except dairy footwear except rubber fuel and ice retailing furniture and fixtures furniture and house furnishings stores gas and steam supply systems gasoline service stations general merchandise glass and glass products grain mill products hardware and farm implement stores hospitals hotels and lodging places household appliance and radio stores insurance jewelry stores knitting mills laundering cleaning and dyeing leather products except footwear legal services liquor stores local public administration lumber and building material retailing machinery equipment and supplies meat products medical and other health services except hospitals misc business services misc chemicals and allied products misc entertainment and recreation services misc fabricated textile products misc food preparations and kindred products misc machinery misc manufacturing industries misc nonmetallic mineral and stone products misc paper and pulp products misc personal services misc professional and related misc repair services misc retail stores misc textile mill products misc wholesale trade misc wood products motor vehicles and accessories retailing motor vehicles and equipment motor vehicles and motor vehicle equipment nonmettalic mining and quarrying except fuel nonprofit membership organizs not specified food industries not specified manufacturing industries not specified metal industries not specified retail trade not specified wholesale trade office and store machines other and not specified utilities other primary iron and steel industries paints varnishes and related products paperboard containers and boxes petroleum products petroleum refining photographic equipment and supplies postal service pottery and related prods primary nonferrous industries printing publishing and allied industries private households professional equipment pulp paper and paper board mills radio broadcasting and television railroad and misc transportation equipment railroads and railway real estate retail florists rubber products sanitary services sawmills planing mills and mill work security and commodity brokerage and invest companies services incidental to transportation ship and boat building and repairing shoe repair shops shoe stores state public administration street railways and bus lines structural clay products taxicab service telegraph telephone theaters and motion pictures tobacco manufactures trucking service warehousing and storage water supply water transportation welfare and religious services yarn thread and fabric\n",
      "num: 21.0\n",
      "nom: f g s y\n",
      "num: 4.5\n",
      "nom: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      "num: 23.0\n",
      "nom: 0 4 5 6 9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 48 49 50 58 61 63 79\n",
      "num: 22.0\n",
      "nom: 0 16 36 42\n",
      "num: 25.0\n",
      "nom: 0 5 10 15 20 25 30 35 40 45 50 65 70\n",
      "num: 26.0\n",
      "nom: 0 9 55 124 165\n",
      "num: 24.0\n",
      "nom: postoperative patient data\n",
      "num: 28.0\n",
      "nom: profb\n",
      "num: 27.0\n",
      "nom: 0 1 2 3 4 5 6 7 8 12 16 24 26 32 52\n",
      "num: 4.625\n",
      "nom: 0 39\n",
      "num: 23.5\n",
      "nom: 0 7 14 21 28 35 49\n",
      "num: 32.0\n",
      "nom:  0 6 1 7 1 9 10 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 11 11 1 11 2 11 3 11 4 11 5 11 6 11 7 11 8 11 9 12 12 1 12 2 12 3 12 4 12 5 12 6 12 7 12 8 12 9 13 13 1 13 2 13 3 13 4 13 6 13 7 13 8 13 9 14 14 1 14 2 14 3 14 4 14 5 14 6 14 7 14 8 14 9 15 1 15 2 15 3 15 4 15 5 15 6 15 7 15 8 15 9 16 1 16 2 16 3 16 4 16 5 16 6 16 7 16 8 16 9 17 17 1 17 2 17 3 17 4 17 5 17 6 17 7 17 8 17 9 18 18 1 18 2 18 3 18 4 18 5 18 6 18 7 18 8 18 9 19 19 1 19 2 19 3 19 4 19 5 19 6 19 7 19 8 19 9 2 2 2 3 2 4 2 7 2 9 20 20 1 20 2 20 3 20 4 20 5 20 6 20 7 20 8 20 9 21 21 1 21 2 21 3 21 4 21 5 21 6 21 7 21 8 21 9 22 22 1 22 2 22 3 22 4 22 5 22 6 22 7 22 8 22 9 23 23 1 23 2 23 3 23 4 23 5 23 6 23 7 23 8 23 9 24 24 1 24 2 24 3 24 4 24 5 24 6 24 7 24 8 24 9 25 25 1 25 2 25 3 25 4 25 5 25 6 25 7 25 8 25 9 26 26 1 26 2 26 3 26 4 26 5 26 6 26 7 26 8 26 9 27 27 1 27 2 27 3 27 4 27 5 27 6 27 7 27 8 27 9 28 28 1 28 2 28 3 28 4 28 5 28 6 28 7 28 8 28 9 29 29 1 29 2 29 3 29 4 29 5 29 6 29 7 29 8 29 9 3 3 5 3 6 30 30 1 30 2 30 3 30 4 30 5 30 6 30 7 30 8 30 9 31 31 1 31 2 31 3 31 4 31 5 31 6 31 7 31 8 31 9 32 32 1 32 2 32 3 32 4 32 5 32 6 32 7 32 8 32 9 33 33 1 33 2 33 3 33 4 33 5 33 6 33 7 33 8 33 9 34 34 1 34 2 34 3 34 4 34 5 34 6 34 7 34 8 34 9 35 35 1 35 2 35 3 35 4 35 6 35 7 35 8 36 36 1 36 3 36 4 36 5 36 6 36 7 36 9 37 1 37 2 37 7 37 8 37 9 38 38 1 38 5 38 6 38 9 39 4 39 6 39 9 4 1 4 4 4 7 4 9 5 5 1 5 2 5 3 5 4 5 5 5 6 5 7 5 8 5 9 6 1 6 3 6 5 6 6 6 7 6 9 7 2 7 3 7 4 7 6 7 7 7 9 8 8 1 8 2 8 3 8 4 8 7 8 8 8 9 9 1 9 2 9 3 9 4 9 6 9 7 9 8 9 9\n",
      "num: 33.0\n",
      "nom: 245 269 5 294 318 5 343 367 5 416 5\n",
      "num: 34.0\n",
      "nom: 23779 31010 32137 117877 117883 117891 117894 117903 117911 117917 117919 117923 117927 117930 117933 117936 117940 117944 117952 117954 117960 117962 117969 117976 117979 117981 117984 117990 117994 118001 118004 118007 118011 118024 118026 118041 118052 118076 118080 118085 118091 118096 118102 118107 118115 118121 118124 118139 118150 118164 118170 118178 118182 118193 118201 118213 118217 118220 118225 118237 118257 118266 118270 118291 118300 118316 118327 118340 118343 118350 118359 118386 118413 118442 118446 118463 118491 118542 118551 118574 118580 118583 118587 118596 118603 118659 118671 118718 118726 118743 118753 118775 118855 118888 118907 118954 118977 118991 119028 119063 119070 119075 119091 119135 119171 119179 119216 119256 119281 119302 119344 119370 119403 119428 119597 119616 119623 119666 119692 119715 119741 119762 119763 119829 119836 119883 119921 120018 120141 120216 120269 120343 120355 120811 120846 120862 120865 120884 121006 121013 121019 121519 121602 121786 122533 122974 123330 123999 124035 124157 124335 125018 125100 125715 126095 126102 126919 126975 127045 130600 130685 131390 131854 132564 132840 138799 140550 141176 141222 143009 145248 147237 151110 159716 176316 185842 286791\n",
      "num: 35.0\n",
      "nom: 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6100 6200 6300 6400 6500 6600 6700 6800 7000 7200 7300 7400 7900\n",
      "num: 36.0\n",
      "nom: wind correlations\n",
      "num: 37.0\n",
      "nom: spambase\n",
      "num: 37.5\n",
      "nom: black blue gold green orange red white\n",
      "num: 39.0\n",
      "nom:  0 8 1 2 1 3 1 4 1 6 1 7 1 8 1 9 2 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 3 3 1 3 2 3 3 3 4 3 5 3 6 3 7 3 8 3 9 4 4 1 4 2 4 3 4 4 4 5 4 6 4 7 4 8 4 9 5 5 1 5 2 5 3 5 4 5 5 5 6 5 7 5 8 5 9 6 6 1 6 2 6 3 6 4 6 5 6 6 6 7 6 8 6 9 7 7 1 7 2 7 3 7 4 7 5 7 6 7 7 7 9 8 8 1 8 3 8 5 8 9 9 3 9 6\n",
      "num: 35.875\n",
      "nom: delta elevators\n",
      "num: 40.0\n",
      "nom: detroit\n",
      "num: 36.125\n",
      "nom: a c f l m n p s y\n",
      "num: 7.5\n",
      "nom: 0 4 7 10 15 65 84 101\n",
      "num: 36.5\n",
      "nom: 0 15\n",
      "num: 7.625\n",
      "nom: autumn spring summer winter\n",
      "num: 40.125\n",
      "nom: robot failures lp3\n",
      "num: 47.0\n",
      "nom: 0 7 9 10 64 210 251\n",
      "num: 48.0\n",
      "nom: 0 0 27 1 1 09 1 13 1 14 1 2 1 25 1 33 1 45 1 5 1 67 1 75 2 3\n",
      "num: 49.0\n",
      "nom: yes no\n",
      "num: 50.0233568933578\n",
      "nom: 0 8 16 24 32\n",
      "num: 51.0\n",
      "nom: 2 3 4 5\n",
      "num: 52.0\n",
      "nom: tamilnadu electricity\n",
      "num: 50.711045533115\n",
      "nom: 0 41 51 70 243 247\n",
      "num: 50.0\n",
      "nom: puma32h\n",
      "num: 52.4575163398693\n",
      "nom: 2 1 3\n",
      "num: 11.0\n",
      "nom:  0 01 0 02 0 04 0 05 0 06 0 07 0 08 0 09 0 1 0 14 0 15 0 16 0 17 0 19 0 23 0 24 0 25 0 26 0 27 0 28 0 29 0 3 0 32 0 33 0 35 0 37 0 42 0 43 0 44 0 45 0 47 0 48 0 49 0 5 0 51 0 52 0 53 0 54 0 55 0 56 0 57 0 58 0 6 0 61 0 62 0 63 0 64 0 65 0 66 0 67 0 69 0 7 0 71 0 72 0 74 0 75 0 76 0 79 0 8 0 81 0 83 0 84 0 85 0 86 0 87 0 88 0 89 0 91 0 92 0 93 0 94 0 95 0 96 0 97 1 1 01 1 03 1 04 1 05 1 06 1 07 1 08 1 09 1 1 1 11 1 12 1 13 1 14 1 15 1 16 1 19 1 2 1 22 1 23 1 24 1 25 1 26 1 27 1 28 1 29 1 3 1 31 1 32 1 34 1 35 1 36 1 38 1 4 1 41 1 42 1 44 1 45 1 46 1 47 1 48 1 49 1 5 1 51 1 52 1 54 1 55 1 56 1 57 1 58 1 59 1 62 1 64 1 65 1 66 1 67 1 68 1 7 1 72 1 73 1 75 1 76 1 77 1 78 1 79 1 81 1 82 1 83 1 84 1 85 1 86 1 87 1 88 1 89 1 9 1 91 1 93 1 94 1 95 1 96 1 98 1 99 10 10 02 10 04 10 15 10 24 10 28 10 34 10 41 10 42 10 49 10 64 10 67 10 69 10 74 10 75 10 8 10 86 10 88 10 89 10 95 10 96 10 99 11 04 11 05 11 09 11 13 11 14 11 18 11 21 11 23 11 25 11 27 11 28 11 34 11 36 11 38 11 41 11 46 11 49 11 51 11 52 11 56 11 58 11 59 11 65 11 8 11 84 11 85 11 91 11 93 11 94 11 98 12 01 12 07 12 09 12 13 12 17 12 19 12 25 12 27 12 31 12 33 12 37 12 49 12 58 12 71 12 96 13 15 13 27 13 3 13 31 13 35 13 47 13 5 13 6 13 61 13 64 13 73 13 8 13 94 14 03 14 27 14 36 14 41 14 44 14 64 14 66 14 67 14 69 14 9 14 95 14 97 14 98 15 02 15 68 15 85 16 16 24 16 33 16 43 16 64 17 19 17 45 17 85 17 99 18 12 18 17 18 58 18 95 19 47 19 49 19 94 2 2 01 2 02 2 05 2 06 2 07 2 1 2 11 2 12 2 14 2 16 2 18 2 19 2 2 2 21 2 22 2 23 2 25 2 27 2 28 2 31 2 32 2 33 2 35 2 36 2 37 2 38 2 42 2 43 2 46 2 47 2 49 2 52 2 53 2 54 2 55 2 57 2 58 2 59 2 6 2 63 2 64 2 65 2 66 2 67 2 69 2 7 2 71 2 72 2 73 2 74 2 76 2 78 2 79 2 8 2 81 2 82 2 83 2 84 2 86 2 9 2 91 2 93 2 94 2 95 2 96 2 97 2 98 2 99 20 33 21 28 22 18 22 48 22 99 25 99 3 01 3 03 3 04 3 06 3 07 3 08 3 1 3 14 3 15 3 16 3 18 3 2 3 21 3 24 3 26 3 29 3 31 3 32 3 33 3 34 3 35 3 36 3 37 3 38 3 41 3 42 3 43 3 44 3 45 3 46 3 47 3 48 3 49 3 5 3 52 3 53 3 54 3 55 3 56 3 57 3 58 3 59 3 61 3 63 3 64 3 71 3 72 3 77 3 79 3 8 3 81 3 83 3 84 3 85 3 86 3 87 3 88 3 89 3 9 3 91 3 93 3 94 3 96 3 98 3 99 4 4 01 4 02 4 03 4 04 4 05 4 07 4 08 4 09 4 1 4 11 4 12 4 14 4 15 4 16 4 17 4 18 4 2 4 22 4 25 4 26 4 28 4 3 4 31 4 32 4 33 4 35 4 38 4 4 4 41 4 43 4 44 4 45 4 49 4 5 4 51 4 52 4 53 4 56 4 59 4 63 4 64 4 67 4 68 4 69 4 76 4 78 4 79 4 8 4 81 4 83 4 84 4 88 4 89 4 92 4 95 4 96 4 97 4 98 4 99 5 01 5 02 5 03 5 07 5 08 5 09 5 11 5 12 5 14 5 16 5 17 5 19 5 21 5 22 5 23 5 24 5 26 5 31 5 32 5 33 5 34 5 38 5 39 5 45 5 46 5 48 5 49 5 5 5 51 5 53 5 54 5 55 5 57 5 58 5 6 5 64 5 67 5 69 5 74 5 75 5 76 5 77 5 79 5 8 5 82 5 83 5 84 5 86 5 87 5 88 5 89 5 9 5 91 5 92 5 95 5 96 5 97 5 98 5 99 6 6 01 6 06 6 07 6 08 6 09 6 1 6 12 6 15 6 16 6 17 6 18 6 19 6 2 6 21 6 24 6 25 6 28 6 32 6 34 6 35 6 38 6 39 6 42 6 43 6 47 6 5 6 51 6 53 6 54 6 57 6 58 6 59 6 6 6 61 6 62 6 64 6 65 6 66 6 69 6 7 6 75 6 76 6 77 6 78 6 81 6 84 6 86 6 87 6 91 6 93 6 95 6 96 6 98 7 7 02 7 03 7 05 7 08 7 09 7 11 7 13 7 2 7 22 7 28 7 31 7 32 7 33 7 38 7 39 7 4 7 44 7 45 7 5 7 53 7 58 7 59 7 6 7 62 7 65 7 66 7 67 7 69 7 7 7 71 7 73 7 76 7 78 7 81 7 85 7 86 7 89 7 9 7 92 7 93 7 94 7 97 7 98 8 8 03 8 05 8 06 8 08 8 1 8 11 8 18 8 21 8 22 8 23 8 24 8 25 8 27 8 28 8 3 8 31 8 35 8 36 8 37 8 39 8 42 8 43 8 52 8 53 8 55 8 56 8 58 8 59 8 62 8 65 8 66 8 67 8 71 8 73 8 82 8 83 8 84 8 86 8 87 8 88 8 89 8 9 8 93 9 01 9 04 9 06 9 08 9 13 9 16 9 17 9 18 9 21 9 22 9 31 9 36 9 39 9 43 9 49 9 57 9 58 9 59 9 6 9 61 9 64 9 65 9 66 9 67 9 71 9 73 9 76 9 78 9 84 9 88 9 93 9 94 9 98 0 0 02 0 04 0 05 0 07 0 09 0 1 0 12 0 13 0 14 0 16 0 18 0 2 0 21 0 22 0 23 0 24 0 25 0 26 0 27 0 29 0 3 0 31 0 34 0 35 0 36 0 38 0 4 0 41 0 42 0 43 0 44 0 45 0 46 0 47 0 49 0 5 0 51 0 53 0 55 0 56 0 57 0 58 0 6 0 61 0 62 0 63 0 64 0 66 0 67 0 68 0 69 0 7 0 71 0 72 0 73 0 76 0 79 0 8 0 83 0 84 0 85 0 86 0 87 0 88 0 89 0 91 0 92 0 93 0 94 0 95 0 96 0 97 0 98 1 1 01 1 05 1 06 1 07 1 08 1 09 1 1 1 13 1 14 1 16 1 17 1 18 1 2 1 21 1 23 1 24 1 25 1 26 1 28 1 29 1 31 1 32 1 33 1 34 1 35 1 36 1 37 1 38 1 39 1 4 1 41 1 43 1 46 1 48 1 5 1 51 1 52 1 53 1 54 1 55 1 57 1 59 1 61 1 62 1 63 1 66 1 67 1 68 1 69 1 7 1 71 1 73 1 74 1 75 1 77 1 79 1 8 1 83 1 84 1 85 1 86 1 88 1 89 1 9 1 91 1 93 1 94 1 97 1 98 10 01 10 02 10 05 10 06 10 08 10 11 10 24 10 25 10 26 10 27 10 28 10 3 10 37 10 4 10 41 10 43 10 44 10 45 10 47 10 49 10 51 10 54 10 55 10 57 10 59 10 64 10 65 10 67 10 71 10 74 10 79 10 8 10 81 10 85 10 88 10 89 10 95 10 98 11 11 04 11 09 11 12 11 19 11 2 11 22 11 24 11 28 11 3 11 36 11 41 11 45 11 5 11 56 11 65 11 66 11 73 11 74 11 75 11 83 11 89 11 9 11 91 11 94 11 96 11 97 12 01 12 02 12 06 12 07 12 1 12 16 12 18 12 19 12 2 12 23 12 24 12 26 12 27 12 29 12 39 12 4 12 41 12 43 12 45 12 5 12 58 12 59 12 66 12 77 12 8 12 82 12 85 12 97 12 98 13 03 13 08 13 11 13 12 13 13 13 18 13 21 13 22 13 27 13 29 13 35 13 42 13 48 13 52 13 53 13 65 13 67 13 71 13 83 13 9 13 96 14 03 14 06 14 1 14 17 14 27 14 37 14 41 14 53 14 56 14 57 14 72 14 74 14 8 14 81 14 82 14 85 14 86 14 89 15 15 01 15 03 15 05 15 07 15 1 15 14 15 25 15 31 15 33 15 34 15 39 15 44 15 6 15 63 15 65 15 84 15 86 15 92 16 15 16 22 16 25 16 35 16 43 16 63 16 66 16 67 16 76 16 88 16 92 17 11 17 18 17 22 17 36 17 47 17 5 17 56 17 59 17 65 17 73 17 75 17 78 17 86 17 91 17 95 18 04 18 16 18 17 18 21 18 43 18 49 18 52 18 55 18 64 18 7 18 88 19 01 19 35 19 43 19 64 19 67 19 82 19 92 2 2 02 2 04 2 05 2 06 2 09 2 1 2 12 2 13 2 14 2 15 2 18 2 19 2 2 2 22 2 23 2 24 2 25 2 27 2 28 2 29 2 3 2 32 2 33 2 34 2 36 2 38 2 39 2 41 2 42 2 43 2 44 2 46 2 48 2 49 2 5 2 51 2 52 2 53 2 56 2 58 2 59 2 6 2 61 2 62 2 63 2 64 2 67 2 68 2 69 2 7 2 71 2 73 2 75 2 76 2 77 2 78 2 81 2 82 2 83 2 85 2 86 2 87 2 88 2 89 2 9 2 92 2 95 2 98 2 99 20 02 20 35 20 51 20 55 20 6 20 84 20 86 21 04 21 5 21 53 21 62 21 78 22 05 22 17 22 28 22 53 22 54 22 73 23 04 23 26 24 38 24 93 25 6 27 01 27 54 28 04 28 39 3 3 01 3 02 3 03 3 04 3 06 3 07 3 08 3 09 3 11 3 13 3 14 3 16 3 18 3 19 3 2 3 21 3 22 3 23 3 24 3 26 3 27 3 29 3 31 3 33 3 35 3 36 3 39 3 4 3 41 3 42 3 43 3 44 3 45 3 46 3 47 3 48 3 49 3 5 3 53 3 54 3 55 3 57 3 59 3 63 3 65 3 66 3 67 3 68 3 71 3 72 3 76 3 77 3 78 3 81 3 82 3 84 3 85 3 86 3 87 3 88 3 89 3 9 3 92 3 93 3 94 3 95 3 97 3 98 30 42 4 4 01 4 03 4 04 4 05 4 06 4 07 4 08 4 09 4 1 4 11 4 14 4 15 4 16 4 19 4 2 4 23 4 25 4 26 4 27 4 28 4 29 4 3 4 31 4 35 4 37 4 38 4 39 4 4 4 41 4 42 4 47 4 48 4 49 4 5 4 52 4 56 4 57 4 58 4 59 4 6 4 62 4 63 4 66 4 67 4 68 4 71 4 74 4 76 4 77 4 78 4 79 4 82 4 83 4 84 4 85 4 86 4 89 4 9 4 91 4 93 4 94 4 95 4 96 4 99 5 5 01 5 02 5 03 5 05 5 07 5 09 5 13 5 14 5 15 5 16 5 18 5 19 5 2 5 23 5 24 5 27 5 31 5 32 5 34 5 37 5 38 5 4 5 41 5 42 5 44 5 48 5 53 5 54 5 56 5 58 5 59 5 6 5 61 5 62 5 63 5 65 5 67 5 69 5 71 5 72 5 73 5 75 5 76 5 77 5 78 5 79 5 82 5 83 5 84 5 86 5 87 5 89 5 9 5 91 5 92 5 93 5 96 5 99 6 03 6 04 6 05 6 06 6 08 6 13 6 14 6 16 6 17 6 18 6 19 6 2 6 21 6 23 6 3 6 31 6 34 6 35 6 37 6 38 6 39 6 42 6 43 6 46 6 49 6 57 6 6 6 61 6 63 6 64 6 66 6 67 6 69 6 7 6 71 6 75 6 76 6 78 6 79 6 8 6 82 6 83 6 84 6 87 6 89 6 92 6 93 7 01 7 04 7 05 7 06 7 07 7 08 7 1 7 12 7 13 7 14 7 15 7 23 7 25 7 29 7 31 7 32 7 37 7 38 7 39 7 4 7 41 7 42 7 48 7 49 7 53 7 56 7 57 7 58 7 59 7 6 7 62 7 64 7 65 7 66 7 71 7 72 7 73 7 74 7 78 7 81 7 82 7 85 7 86 7 87 7 89 7 9 7 91 7 92 7 94 7 95 7 96 7 97 8 8 02 8 08 8 1 8 12 8 13 8 14 8 17 8 18 8 19 8 2 8 22 8 27 8 31 8 34 8 36 8 37 8 45 8 49 8 51 8 52 8 54 8 56 8 59 8 6 8 62 8 63 8 64 8 65 8 75 8 83 8 85 8 86 8 88 8 91 8 93 8 95 8 97 9 04 9 05 9 07 9 08 9 1 9 13 9 14 9 15 9 16 9 17 9 19 9 22 9 24 9 28 9 3 9 31 9 37 9 39 9 46 9 53 9 55 9 57 9 6 9 61 9 62 9 63 9 65 9 66 9 67 9 69 9 7 9 71 9 77 9 79 9 8 9 82 9 89 9 95 9 98 9 99\n",
      "num: 10.875\n",
      "nom: self employed works for wages salary\n",
      "num: 52.5\n",
      "nom: build wind float build wind non float vehic wind float vehic wind non float containers tableware headlamps\n",
      "num: 10.5\n",
      "nom: 0 1 2 4 5\n",
      "num: 53.0\n",
      "nom: g p gg\n",
      "num: 61.0\n",
      "nom: 1 2 3 4 5 6 7\n",
      "num: 62.5\n",
      "nom: 0 1 3\n",
      "num: 63.0\n",
      "nom: robot failures lp4\n",
      "num: 64.0\n",
      "nom:  00041 00030 00013 00012 00009 00007 00006 00004 00002 0 50 150 250 350 450 550 650 750 850 950 1050 1150 1250 1350 1450 1550 1650 1750 1850 1950 2050 2150 2250 2350 2450 2550 2650 2750 2850 2950 3050 3150 3250 3350 3450 3550 3650 3750 3850 3950 4050 4150 4250 4350 4450 4550 4650 4750 4850 4950 5050 5150 5250 5350 5450 5550 5650 5750 5850 5950 6050 6150 6250 6350 6450 6550 6650 6750 6850 6950 7050 7150 7250 7350 7450 7550 7650 7750 7850 7950 8050 8150 8250 8350 8450 8550 8650 8750 8850 8950 9050 9150 9250 9350 9450 9550 9650 9750 9850 9950 10050 10150 10250 10350 10450 10550 10650 10750 10850 10950 11050 11150 11250 11350 11450 11550 11650 11750 11850 11950 12050 12150 12250 12350 12450 12550 12650 12750 12850 12950 13050 13150 13250 13350 13450 13550 13650 13750 13850 13950 14050 14150 14250 14350 14450 14550 14650 14750 14850 15050 15150 15250 15350 15450 15550 15650 15750 15850 15950 16050 16150 16250 16350 16450 16550 16650 16750 16850 16950 17050 17150 17250 17350 17450 17550 17650 17750 17850 17950 18050 18150 18250 18350 18450 18650 18750 18950 19050 19150 19250 19450 19550 19650 19850 20050 20150 20450 20550 20850 20950 21050 21150 21350 21550 21650 21750 21850 22050 22150 22250 22350 22550 22650 22950 23050 23150 23250 23350 23450 23750 24050 24150 24350 24550 25050 25150 26150 26250 26350 26450 27050 27150 27250 27350 27550 28050 28150 28950 29150 29450 29650 30050 30150 30650 30750 30850 30950 31150 32150 32650 32950 33350 33550 34050 34150 35050 35150 36650 37650 38150 38850 39150 39550 40050 40650 41150 42050 42150 42550 43950 45150 45650 46550 48050 48250 49050 50000 999999\n",
      "num: 65.0\n",
      "nom: wife own child husband not in family other relative unmarried\n",
      "num: 13.0\n",
      "nom: banknote authentication\n",
      "num: 13.75\n",
      "nom: esl\n",
      "num: 12.5\n",
      "nom: 0 79 128 171 240 255\n",
      "num: 60.0\n",
      "nom: kdd coil 2\n",
      "num: 62.0\n",
      "nom: walking activity\n",
      "num: 14.0\n",
      "nom: well fairly poorly\n",
      "num: 72.9565324905791\n",
      "nom:  0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 3 3 1 3 2 3 3 3 4 3 5 3 6 3 7 3 8 3 9 4 4 1 4 2 4 3 4 4 4 5 4 6 4 7 4 8 4 9 5 5 1 5 2 5 3 5 4 5 5 5 6 5 7 5 8 5 9 6 6 1 6 2 6 3 6 5 6 7 7 7 1 8 4 9 2\n",
      "num: 72.6509345794392\n",
      "nom:  0 01 0 02 0 03 0 04 0 05 0 06 0 07 0 08 0 09 0 1 0 11 0 12 0 13 0 14 0 15 0 17 0 18 0 19 0 2 0 22 0 23 0 24 0 25 0 26 0 27 0 28 0 29 0 3 0 31 0 34 0 35 0 37 0 38 0 39 0 41 0 42 0 43 0 47 0 48 0 49 0 5 0 51 0 52 0 53 0 54 0 55 0 57 0 58 0 59 0 6 0 63 0 64 0 65 0 66 0 67 0 68 0 69 0 7 0 71 0 73 0 74 0 75 0 76 0 78 0 79 0 8 0 81 0 83 0 84 0 85 0 87 0 88 0 9 0 91 0 92 0 93 0 96 0 97 0 98 1 1 01 1 02 1 03 1 04 1 06 1 07 1 08 1 09 1 1 1 11 1 12 1 13 1 14 1 15 1 16 1 19 1 2 1 21 1 22 1 24 1 26 1 27 1 28 1 29 1 3 1 31 1 32 1 33 1 36 1 37 1 4 1 41 1 42 1 43 1 44 1 45 1 46 1 47 1 48 1 49 1 5 1 52 1 54 1 56 1 58 1 59 1 6 1 61 1 62 1 63 1 65 1 68 1 69 1 7 1 71 1 72 1 73 1 75 1 76 1 77 1 78 1 79 1 8 1 81 1 82 1 85 1 86 1 87 1 88 1 9 1 92 1 93 1 94 1 95 1 96 1 97 1 99 10 02 10 03 10 08 10 09 10 11 10 13 10 17 10 21 10 23 10 27 10 29 10 39 10 42 10 43 10 45 10 48 10 5 10 52 10 57 10 59 10 6 10 61 10 64 10 68 10 7 10 79 10 8 10 86 10 87 10 95 10 96 11 01 11 17 11 23 11 25 11 27 11 37 11 39 11 52 11 58 11 65 11 9 11 93 12 05 12 19 12 2 12 25 12 29 12 37 12 44 12 52 12 55 12 68 12 69 12 73 12 8 12 98 12 99 13 06 13 13 13 21 13 25 13 28 13 32 13 41 13 42 13 44 13 46 13 67 13 8 13 85 14 14 03 14 05 14 06 14 32 14 64 14 65 14 68 14 87 14 97 15 27 15 37 15 68 15 92 15 95 18 4 18 54 19 45 2 02 2 03 2 04 2 05 2 08 2 09 2 12 2 14 2 17 2 18 2 19 2 2 2 21 2 22 2 23 2 24 2 25 2 26 2 28 2 29 2 3 2 31 2 32 2 34 2 36 2 37 2 38 2 39 2 42 2 43 2 44 2 45 2 46 2 47 2 48 2 49 2 5 2 51 2 52 2 53 2 54 2 55 2 56 2 57 2 58 2 6 2 62 2 63 2 66 2 67 2 68 2 7 2 74 2 77 2 78 2 8 2 82 2 83 2 85 2 86 2 88 2 89 2 9 2 91 2 92 2 93 2 95 2 96 2 97 2 98 21 09 21 11 23 68 3 01 3 03 3 04 3 05 3 06 3 1 3 11 3 12 3 15 3 17 3 19 3 2 3 21 3 22 3 23 3 24 3 26 3 27 3 29 3 31 3 32 3 33 3 34 3 35 3 36 3 38 3 4 3 41 3 42 3 43 3 45 3 47 3 48 3 49 3 5 3 52 3 53 3 54 3 55 3 56 3 57 3 58 3 6 3 61 3 63 3 65 3 67 3 69 3 7 3 71 3 72 3 73 3 74 3 77 3 78 3 79 3 8 3 82 3 83 3 84 3 85 3 86 3 87 3 88 3 9 3 91 3 92 3 94 3 97 3 98 3 99 4 4 01 4 02 4 04 4 06 4 07 4 09 4 1 4 13 4 15 4 17 4 18 4 2 4 21 4 22 4 23 4 24 4 25 4 26 4 28 4 29 4 3 4 31 4 33 4 35 4 36 4 37 4 38 4 42 4 44 4 46 4 48 4 49 4 51 4 52 4 53 4 54 4 55 4 57 4 58 4 59 4 62 4 63 4 66 4 69 4 71 4 74 4 75 4 76 4 78 4 8 4 83 4 87 4 88 4 89 4 9 4 91 4 92 4 93 4 94 4 95 5 02 5 04 5 05 5 07 5 09 5 1 5 11 5 13 5 14 5 17 5 19 5 2 5 23 5 27 5 3 5 31 5 32 5 33 5 34 5 37 5 41 5 43 5 44 5 48 5 54 5 56 5 58 5 59 5 62 5 63 5 64 5 65 5 67 5 7 5 71 5 72 5 77 5 78 5 79 5 84 5 86 5 87 5 88 5 89 5 9 5 91 6 02 6 05 6 06 6 07 6 1 6 11 6 14 6 17 6 18 6 24 6 25 6 26 6 28 6 3 6 32 6 34 6 35 6 41 6 43 6 46 6 47 6 5 6 52 6 55 6 56 6 6 6 63 6 66 6 67 6 68 6 69 6 71 6 74 6 78 6 8 6 81 6 82 6 83 6 84 6 88 6 9 6 93 6 96 6 97 7 7 01 7 02 7 04 7 06 7 08 7 12 7 28 7 31 7 33 7 34 7 37 7 39 7 41 7 44 7 45 7 46 7 48 7 5 7 6 7 61 7 65 7 66 7 68 7 7 7 74 7 75 7 77 7 78 7 79 7 8 7 81 7 88 7 89 7 9 7 92 7 95 7 98 7 99 8 03 8 04 8 05 8 1 8 15 8 18 8 19 8 23 8 31 8 33 8 36 8 37 8 41 8 42 8 43 8 44 8 45 8 5 8 58 8 59 8 65 8 67 8 68 8 69 8 7 8 72 8 75 8 76 8 82 8 85 8 91 9 11 9 13 9 14 9 24 9 25 9 26 9 31 9 33 9 34 9 39 9 45 9 46 9 57 9 58 9 61 9 62 9 73 9 78 9 82 9 83 9 84 9 86 9 9 9 98 0 0 02 0 03 0 04 0 05 0 06 0 07 0 09 0 1 0 11 0 12 0 13 0 14 0 15 0 16 0 18 0 19 0 2 0 21 0 22 0 23 0 25 0 26 0 27 0 28 0 29 0 3 0 31 0 32 0 33 0 34 0 35 0 36 0 38 0 4 0 41 0 42 0 43 0 44 0 46 0 47 0 48 0 49 0 5 0 51 0 53 0 54 0 56 0 57 0 58 0 6 0 61 0 62 0 63 0 65 0 66 0 67 0 68 0 7 0 71 0 73 0 74 0 76 0 78 0 8 0 81 0 82 0 83 0 85 0 86 0 87 0 88 0 89 0 9 0 92 0 93 0 94 0 95 0 96 0 97 0 98 0 99 1 1 01 1 02 1 03 1 05 1 06 1 07 1 08 1 09 1 1 1 11 1 12 1 13 1 14 1 15 1 16 1 17 1 18 1 19 1 21 1 22 1 23 1 25 1 26 1 28 1 29 1 3 1 31 1 32 1 33 1 34 1 36 1 39 1 4 1 41 1 42 1 43 1 44 1 45 1 46 1 47 1 48 1 49 1 5 1 51 1 52 1 53 1 55 1 56 1 57 1 59 1 6 1 61 1 63 1 64 1 65 1 66 1 68 1 69 1 7 1 71 1 72 1 73 1 74 1 75 1 76 1 78 1 79 1 8 1 81 1 82 1 84 1 86 1 87 1 88 1 9 1 91 1 92 1 93 1 94 1 95 1 98 1 99 10 10 01 10 03 10 05 10 08 10 09 10 1 10 12 10 13 10 16 10 17 10 27 10 28 10 32 10 34 10 35 10 37 10 4 10 44 10 45 10 5 10 52 10 54 10 57 10 59 10 64 10 69 10 7 10 79 10 81 10 83 10 84 10 92 10 94 10 99 11 11 01 11 03 11 06 11 09 11 14 11 24 11 26 11 28 11 32 11 34 11 36 11 39 11 41 11 42 11 43 11 46 11 48 11 64 11 65 11 71 11 8 11 86 11 88 11 89 11 92 11 97 11 99 12 01 12 03 12 04 12 14 12 19 12 21 12 22 12 23 12 25 12 26 12 33 12 41 12 45 12 48 12 52 12 55 12 57 12 68 12 85 12 88 12 92 12 99 13 13 01 13 06 13 07 13 15 13 16 13 19 13 21 13 23 13 26 13 28 13 3 13 32 13 47 13 48 13 49 13 54 13 6 13 64 13 65 13 74 13 84 13 95 13 99 14 08 14 14 14 15 14 23 14 29 14 32 14 35 14 51 14 59 14 61 14 74 14 89 14 94 15 06 15 18 15 28 15 34 15 38 15 49 15 5 15 61 15 65 15 72 15 86 15 93 15 96 16 13 16 15 16 38 16 47 16 72 16 73 16 93 17 17 09 17 27 17 56 17 72 17 77 17 93 18 01 18 59 18 61 18 62 18 75 18 86 19 24 19 3 19 38 2 2 01 2 02 2 03 2 05 2 06 2 07 2 08 2 09 2 1 2 11 2 15 2 16 2 17 2 18 2 19 2 2 2 21 2 23 2 24 2 26 2 29 2 3 2 31 2 32 2 33 2 34 2 36 2 37 2 39 2 4 2 42 2 43 2 44 2 45 2 46 2 47 2 48 2 49 2 5 2 51 2 52 2 53 2 54 2 56 2 57 2 58 2 59 2 6 2 61 2 62 2 63 2 64 2 65 2 67 2 68 2 7 2 72 2 73 2 75 2 78 2 79 2 8 2 84 2 86 2 87 2 89 2 9 2 91 2 93 2 94 2 95 2 96 2 97 2 98 2 99 20 33 22 15 22 95 25 54 3 03 3 04 3 05 3 06 3 08 3 09 3 1 3 12 3 13 3 14 3 15 3 16 3 18 3 21 3 22 3 23 3 24 3 25 3 27 3 28 3 29 3 3 3 31 3 33 3 34 3 35 3 37 3 38 3 4 3 41 3 42 3 43 3 45 3 47 3 49 3 52 3 55 3 56 3 57 3 58 3 59 3 6 3 61 3 62 3 63 3 64 3 65 3 66 3 67 3 69 3 7 3 72 3 73 3 74 3 75 3 76 3 77 3 78 3 79 3 8 3 81 3 84 3 85 3 87 3 88 3 89 3 91 3 92 3 93 3 94 3 95 3 97 3 99 4 01 4 02 4 06 4 07 4 09 4 12 4 13 4 14 4 16 4 2 4 21 4 22 4 23 4 24 4 25 4 28 4 3 4 32 4 33 4 34 4 35 4 38 4 39 4 42 4 43 4 45 4 46 4 47 4 49 4 5 4 51 4 52 4 53 4 54 4 55 4 56 4 57 4 58 4 6 4 61 4 63 4 65 4 67 4 68 4 69 4 7 4 72 4 73 4 77 4 78 4 79 4 8 4 82 4 84 4 85 4 86 4 87 4 88 4 9 4 92 4 95 4 96 4 97 4 98 4 99 5 5 01 5 02 5 03 5 04 5 05 5 06 5 07 5 08 5 09 5 1 5 11 5 12 5 15 5 16 5 17 5 19 5 2 5 21 5 22 5 23 5 24 5 26 5 27 5 3 5 32 5 33 5 34 5 35 5 37 5 39 5 4 5 41 5 42 5 44 5 45 5 48 5 49 5 5 5 51 5 52 5 54 5 56 5 57 5 58 5 62 5 63 5 64 5 65 5 66 5 67 5 69 5 73 5 75 5 76 5 78 5 79 5 8 5 82 5 83 5 84 5 85 5 86 5 88 5 89 5 9 5 93 5 95 5 98 6 6 01 6 02 6 04 6 06 6 07 6 08 6 09 6 11 6 13 6 14 6 15 6 16 6 17 6 2 6 23 6 24 6 25 6 26 6 27 6 31 6 32 6 35 6 36 6 39 6 43 6 46 6 47 6 51 6 52 6 53 6 55 6 61 6 63 6 66 6 67 6 68 6 69 6 7 6 71 6 76 6 78 6 8 6 81 6 84 6 88 6 89 6 9 6 91 6 94 6 95 6 96 6 97 6 98 6 99 7 7 02 7 03 7 06 7 07 7 09 7 1 7 12 7 14 7 18 7 19 7 2 7 21 7 23 7 24 7 27 7 3 7 31 7 32 7 34 7 35 7 36 7 37 7 39 7 41 7 43 7 45 7 47 7 5 7 55 7 57 7 59 7 6 7 61 7 62 7 63 7 66 7 67 7 68 7 76 7 77 7 8 7 81 7 82 7 83 7 86 7 87 7 9 7 92 7 93 7 94 7 97 7 99 8 8 03 8 04 8 05 8 06 8 11 8 16 8 18 8 19 8 2 8 23 8 24 8 25 8 26 8 27 8 36 8 39 8 4 8 43 8 46 8 49 8 54 8 61 8 63 8 64 8 66 8 67 8 69 8 74 8 75 8 78 8 79 8 81 8 84 8 91 8 96 8 97 9 03 9 08 9 1 9 11 9 15 9 16 9 18 9 21 9 22 9 23 9 24 9 25 9 28 9 32 9 33 9 35 9 38 9 4 9 48 9 54 9 61 9 66 9 67 9 71 9 73 9 74 9 76 9 78 9 82 9 83 9 84 9 85 9 89 9 9 9 93 9 95 9 96 9 97 9 99\n",
      "num: 66.0\n",
      "nom:  0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 3 3 1 3 2 3 3 3 4 3 5 3 6 3 7 3 8 3 9 4 4 1 4 2 4 3 4 4 4 5 4 6 4 7 4 8 4 9 5 5 1 5 2 5 5 5 7 5 8 5 9 6 6 2 6 3 6 5 7 7 4\n",
      "num: 75.0\n",
      "nom:  0 4 0 5 0 6 0 7 0 8 0 9 1 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 3 3 1 3 2 3 3 3 4 3 5 3 6 3 7 3 8 3 9 4 4 1 4 2 4 3 4 4 4 5 4 6 4 7 4 8 4 9 5 5 1 5 2 5 5 5 8 5 9 6 6 1 6 4\n",
      "num: 76.0\n",
      "nom: 5000 12500 16250 18750 21250 23750 26250 28750 32500 37500 42499 47500 52500 57500 62500 67500 72500 77500 85000 95000 112500 137500 175000 200000 999999\n",
      "num: 77.0\n",
      "nom: 0 9\n",
      "num: 15.5\n",
      "nom: 0 8 38 47\n",
      "num: 70.0\n",
      "nom: b c e g n o p w y\n",
      "num: 80.0\n",
      "nom:  0 1 0 2 0 4 0 5 0 6 0 7 0 8 1 1 1 5 1 7 1 9 2 2 2 3 2 7 3 6 0 0 3 0 4 0 5 0 6 0 8 0 9 1 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 10 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 11 11 1 11 2 11 3 11 4 11 5 11 6 11 7 11 8 11 9 12 12 1 12 2 12 3 12 4 12 5 12 6 12 7 12 8 12 9 13 13 1 13 2 13 3 13 4 13 5 13 6 13 7 13 8 13 9 14 14 1 14 2 14 3 14 4 14 5 14 6 14 7 14 8 14 9 15 15 1 15 2 15 3 15 4 15 5 15 6 15 7 15 8 15 9 16 16 1 16 2 16 3 16 4 16 5 16 6 16 7 16 8 16 9 17 17 1 17 2 17 3 17 4 17 5 17 6 17 7 17 8 17 9 18 18 1 18 2 18 3 18 4 18 5 18 6 18 7 18 8 18 9 19 19 1 19 2 19 3 19 4 19 5 19 6 19 7 19 8 19 9 2 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 20 20 1 20 2 20 3 20 4 20 5 20 6 20 7 20 8 20 9 21 21 1 21 2 21 3 21 4 21 5 21 6 21 7 21 8 21 9 22 22 1 22 2 22 3 22 4 22 5 22 6 22 7 22 8 22 9 23 23 1 23 2 23 3 23 4 23 5 23 6 23 7 23 8 23 9 24 24 1 24 2 24 3 24 4 24 5 24 6 24 7 24 8 24 9 25 25 1 25 2 25 3 25 4 25 5 25 6 25 7 25 8 25 9 26 26 1 26 2 26 3 26 4 26 5 26 6 26 7 26 8 26 9 27 27 1 27 2 27 3 27 4 27 5 27 6 27 7 27 8 27 9 28 28 2 3 3 1 3 2 3 3 3 4 3 5 3 6 3 7 3 8 3 9 4 4 1 4 2 4 3 4 4 4 5 4 6 4 7 4 8 4 9 5 1 5 2 5 3 5 4 5 6 5 7 5 8 5 9 6 6 1 6 2 6 3 6 4 6 5 6 6 6 7 6 8 6 9 7 7 1 7 2 7 3 7 4 7 5 7 6 7 7 7 8 7 9 8 8 1 8 2 8 3 8 4 8 5 8 6 8 7 8 8 8 9 9 9 1 9 2 9 3 9 4 9 5 9 6 9 7 9 8 9 9\n",
      "num: 81.25\n",
      "nom:  09215 06005 05485 05005 03900 03380 02325 00835 00685 00505 00345 0 5 15 25 35 45 55 60 65 70 75 95 105 115 125 135 140 155 165 175 185 190 205 210 215 225 230 240 245 250 255 270 275 295 305 310 315 320 325 335 345 355 360 365 375 380 390 395 405 415 425 455 485 505 510 515 520 525 545 555 565 575 580 590 595 605 610 615 625 635 655 705 755 770 785 805 810 815 825 845 855 860 875 885 895 900 905 910 945 955 965 995 1005 1015 1030 1045 1055 1065 1075 1080 1085 1090 1100 1105 1110 1125 1145 1155 1160 1175 1180 1190 1200 1205 1215 1220 1235 1245 1255 1265 1285 1295 1305 1315 1320 1325 1350 1355 1360 1375 1385 1390 1405 1410 1440 1445 1450 1455 1470 1475 1485 1505 1510 1515 1520 1525 1530 1545 1550 1555 1560 1565 1580 1595 1605 1610 1615 1620 1625 1645 1650 1655 1665 1685 1695 1700 1705 1710 1725 1730 1735 1740 1745 1755 1760 1765 1795 1805 1810 1815 1825 1830 1835 1840 1845 1855 1875 1880 1885 1895 1905 1915 1920 1925 1935 1955 1960 1975 1980 1985 1995 2005 2010 2015 2020 2025 2040 2065 2085 2105 2110 2115 2125 2145 2150 2155 2170 2175 2205 2210 2220 2235 2245 2250 2255 2265 2285 2295 2300 2305 2315 2325 2345 2350 2355 2360 2375 2385 2400 2405 2415 2435 2445 2455 2465 2470 2475 2480 2505 2510 2515 2520 2525 2535 2540 2545 2560 2570 2575 2605 2610 2615 2645 2655 2660 2665 2670 2680 2685 2700 2705 2710 2715 2720 2725 2730 2740 2750 2755 2760 2765 2775 2780 2795 2805 2810 2835 2845 2865 2875 2880 2885 2895 2905 2910 2925 2930 2935 2955 2980 2990 3005 3020 3025 3030 3045 3050 3060 3065 3110 3120 3125 3130 3135 3155 3165 3170 3175 3180 3185 3195 3205 3210 3215 3220 3230 3240 3250 3255 3265 3270 3275 3280 3290 3300 3305 3310 3325 3330 3340 3345 3360 3365 3370 3375 3380 3385 3395 3400 3405 3410 3425 3430 3440 3445 3450 3455 3460 3475 3485 3500 3505 3510 3515 3545 3550 3555 3560 3565 3585 3605 3610 3615 3620 3625 3640 3645 3655 3665 3680 3685 3690 3705 3715 3735 3750 3755 3760 3765 3770 3785 3805 3810 3825 3855 3870 3875 3880 3895 3905 3915 3920 3925 3935 3940 3955 3965 3970 3975 3985 3990 3995 4005 4010 4015 4020 4025 4035 4045 4055 4060 4070 4085 4090 4095 4105 4110 4125 4140 4150 4165 4170 4175 4180 4190 4205 4210 4220 4230 4235 4245 4250 4255 4260 4270 4275 4280 4285 4300 4305 4310 4315 4325 4330 4345 4355 4370 4375 4380 4385 4405 4410 4415 4420 4430 4435 4450 4455 4465 4470 4490 4495 4500 4505 4510 4515 4520 4525 4535 4540 4545 4550 4555 4560 4565 4570 4580 4590 4605 4610 4620 4625 4630 4635 4640 4655 4665 4680 4685 4705 4710 4715 4725 4730 4735 4745 4755 4775 4780 4785 4795 4800 4805 4810 4815 4820 4825 4830 4845 4850 4855 4860 4875 4880 4885 4895 4900 4905 4910 4915 4925 4935 4950 4955 4965 4970 4990 5005 5010 5025 5045 5050 5065 5095 5100 5105 5110 5130 5140 5150 5165 5170 5185 5195 5200 5205 5210 5225 5235 5240 5255 5260 5270 5275 5285 5305 5310 5335 5345 5360 5370 5395 5405 5410 5420 5430 5450 5455 5465 5470 5475 5505 5510 5525 5545 5555 5560 5575 5605 5610 5640 5645 5650 5655 5680 5685 5700 5705 5710 5725 5740 5745 5765 5775 5780 5785 5790 5795 5805 5810 5825 5830 5835 5840 5845 5855 5865 5870 5900 5905 5910 5920 5940 5960 5965 5975 5995 6005 6010 6015 6030 6040 6055 6060 6065 6105 6110 6125 6135 6140 6145 6175 6180 6185 6190 6205 6210 6215 6225 6245 6255 6265 6270 6275 6285 6305 6310 6320 6335 6340 6345 6380 6405 6410 6425 6430 6440 6445 6455 6460 6465 6470 6485 6500 6505 6510 6560 6565 6590 6595 6600 6605 6610 6630 6635 6640 6645 6670 6675 6685 6690 6695 6700 6705 6710 6730 6735 6755 6760 6765 6770 6780 6795 6800 6805 6810 6815 6820 6825 6850 6905 6935 6940 6985 6990 7000 7005 7010 7020 7030 7040 7045 7050 7055 7060 7065 7075 7090 7105 7110 7120 7130 7155 7175 7180 7195 7200 7205 7210 7215 7235 7250 7255 7285 7290 7295 7305 7315 7325 7330 7340 7360 7395 7405 7410 7435 7440 7455 7460 7465 7470 7505 7510 7525 7535 7545 7560 7565 7575 7585 7595 7605 7610 7620 7625 7635 7665 7675 7685 7695 7705 7710 7715 7730 7745 7755 7760 7795 7805 7810 7855 7870 7900 7905 7910 7920 7945 7955 7960 7970 7975 7995 8005 8010 8015 8045 8050 8060 8085 8100 8105 8110 8135 8165 8175 8200 8205 8210 8230 8235 8240 8250 8255 8260 8270 8275 8290 8295 8310 8315 8325 8335 8355 8375 8405 8410 8440 8450 8460 8465 8470 8480 8505 8510 8535 8560 8610 8620 8625 8635 8645 8675 8685 8690 8705 8710 8715 8735 8755 8770 8785 8805 8810 8825 8830 8835 8840 8845 8865 8880 8900 8905 8910 8925 8960 8965 8970 8975 8980 8985 8990 8995 9005 9010 9020 9030 9040 9060 9085 9090 9095 9105 9110 9115 9125 9135 9160 9180 9185 9195 9205 9210 9220 9225 9245 9250 9260 9270 9305 9315 9335 9355 9360 9375 9385 9390 9410 9415 9445 9455 9485 9495 9505 9510 9565 9570 9575 9580 9595 9605 9610 9625 9630 9635 9645 9655 9665 9690 9695 9705 9710 9725 9745 9755 9760 9795 9805 9810 9815 9820 9830 9840 9865 9875 9885 9905 9910 9915 9925 9965 9970 9975 9995 10005 10010 10020 10030 10035 10040 10045 10065 10085 10095 10100 10110 10120 10130 10135 10150 10160 10175 10195 10205 10210 10225 10230 10250 10260 10265 10295 10315 10325 10345 10385 10415 10425 10435 10455 10470 10505 10510 10545 10560 10565 10610 10615 10620 10625 10670 10690 10700 10710 10720 10730 10760 10765 10770 10790 10805 10810 10815 10840 10885 10905 10925 10935 10950 10975 10995 11005 11010 11020 11025 11045 11055 11060 11070 11085 11100 11105 11110 11125 11130 11150 11155 11160 11175 11200 11205 11215 11240 11250 11255 11260 11270 11275 11305 11310 11325 11340 11355 11415 11505 11515 11525 11530 11535 11540 11545 11555 11570 11585 11595 11600 11610 11620 11625 11635 11650 11660 11665 11705 11715 11750 11765 11770 11775 11780 11790 11805 11810 11815 11820 11830 11845 11865 11880 11885 11890 11905 11910 11920 11930 11950 11965 12005 12010 12015 12020 12025 12040 12045 12060 12070 12085 12110 12115 12140 12155 12175 12195 12205 12215 12225 12235 12250 12255 12285 12290 12305 12325 12345 12385 12395 12405 12410 12415 12485 12495 12505 12510 12515 12520 12535 12560 12570 12590 12595 12605 12620 12630 12665 12695 12705 12715 12735 12745 12755 12795 12805 12810 12865 12870 12895 12910 12940 12975 12990 13005 13010 13015 13020 13030 13065 13105 13110 13135 13205 13210 13215 13230 13235 13240 13245 13250 13305 13310 13325 13360 13375 13390 13405 13410 13425 13440 13445 13485 13490 13500 13505 13510 13515 13525 13530 13615 13620 13630 13640 13705 13710 13760 13770 13795 13805 13810 13820 13835 13865 13895 13915 13925 13930 13975 13980 13985 14005 14010 14015 14020 14045 14060 14070 14080 14085 14115 14120 14170 14195 14210 14215 14235 14285 14305 14310 14330 14335 14355 14360 14375 14390 14405 14410 14415 14465 14475 14505 14510 14520 14530 14550 14570 14590 14595 14605 14610 14670 14705 14725 14755 14795 14805 14810 14835 14850 14855 14885 14910 14930 14945 14955 14960 14985 14995 15005 15010 15020 15025 15030 15040 15045 15060 15065 15085 15090 15105 15115 15120 15125 15170 15185 15195 15200 15210 15230 15255 15265 15305 15310 15380 15385 15400 15410 15415 15425 15430 15505 15510 15515 15530 15555 15605 15630 15650 15680 15715 15740 15765 15780 15795 15800 15805 15820 15845 15860 15900 15910 15965 15995 16005 16010 16020 16030 16085 16105 16110 16120 16125 16130 16135 16150 16160 16175 16195 16205 16210 16235 16250 16260 16310 16325 16330 16335 16340 16350 16355 16360 16375 16380 16415 16420 16440 16465 16470 16475 16500 16505 16510 16535 16545 16560 16590 16610 16635 16660 16795 16820 16835 16860 16910 16940 17005 17010 17020 17030 17040 17055 17060 17085 17110 17155 17165 17200 17205 17210 17215 17260 17285 17305 17315 17345 17355 17360 17395 17420 17435 17470 17485 17495 17505 17510 17520 17535 17540 17580 17590 17655 17700 17730 17735 17750 17765 17810 17815 17840 17890 17960 18005 18010 18030 18040 18050 18080 18090 18110 18130 18175 18205 18210 18255 18280 18285 18330 18340 18395 18405 18415 18470 18505 18510 18520 18555 18560 18570 18585 18600 18605 18610 18630 18640 18660 18685 18690 18700 18710 18720 18725 18750 18760 18795 18805 18810 18835 18860 18880 18910 18975 19005 19010 19015 19040 19060 19100 19110 19185 19205 19210 19245 19255 19275 19295 19300 19320 19345 19355 19380 19405 19485 19505 19510 19530 19560 19580 19590 19615 19650 19660 19685 19695 19730 19735 19770 19805 19970 20005 20010 20030 20040 20060 20065 20100 20110 20115 20130 20210 20215 20250 20260 20285 20310 20370 20385 20405 20410 20425 20430 20480 20485 20500 20505 20510 20550 20555 20610 20625 20640 20645 20650 20660 20695 20705 20715 20750 20760 20810 20840 20890 20915 20940 21005 21010 21055 21110 21140 21150 21175 21185 21210 21310 21365 21400 21405 21410 21450 21505 21510 21660 21665 21675 21765 21825 21855 21885 21930 22005 22010 22015 22030 22055 22070 22090 22110 22125 22165 22225 22310 22320 22365 22370 22380 22410 22445 22480 22565 22610 22630 22650 22660 22760 22850 22865 22975 23000 23005 23010 23015 23020 23060 23100 23110 23200 23215 23220 23285 23290 23295 23305 23405 23415 23435 23460 23485 23505 23510 23560 23610 23630 23640 23660 23720 23795 23805 23890 23980 23995 24005 24010 24015 24020 24110 24125 24170 24175 24190 24265 24275 24310 24400 24410 24420 24445 24505 24510 24560 24580 24610 24645 24650 24660 24695 24735 24740 24755 24960 24965 24970 24985 25000 25005 25010 25030 25040 25050 25060 25080 25100 25115 25135 25190 25210 25280 25405 25460 25485 25505 25600 25610 25655 25660 25690 25710 25715 25805 25850 25910 25950 25955 25980 26000 26005 26010 26035 26090 26110 26210 26230 26255 26305 26310 26315 26405 26410 26505 26520 26525 26545 26550 26610 26655 26660 26670 26760 26795 26815 26880 26885 26910 26960 27005 27010 27015 27020 27110 27130 27135 27190 27215 27315 27350 27410 27415 27425 27510 27515 27520 27610 27650 27710 27720 27870 27945 28005 28010 28040 28115 28120 28140 28155 28160 28165 28180 28210 28240 28260 28265 28315 28460 28505 28510 28570 28605 28680 28695 28715 28755 28810 28815 28870 29005 29010 29060 29110 29190 29210 29410 29430 29440 29505 29510 29535 29635 29645 29705 29730 29735 29765 29820 29840 29850 29855 29870 29950 30000 30005 30010 30040 30050 30070 30160 30215 30260 30310 30335 30375 30400 30410 30500 30510 30670 30680 30710 30715 30910 31005 31010 31140 31165 31210 31265 31280 31390 31405 31450 31500 31510 31540 31595 31600 31615 31810 31825 31910 32005 32010 32020 32140 32310 32390 32510 32710 32770 32890 32920 33005 33010 33105 33135 33210 33245 33255 33265 33390 33400 33510 33515 33585 33760 33780 33805 33905 34005 34010 34080 34095 34110 34165 34300 34550 34610 34810 34845 34885 35005 35010 35110 35150 35465 35510 35810 35860 35910 36005 36010 36075 36480 36560 36665 36775 36890 36910 37005 37010 37100 37150 37210 37505 37510 38005 38010 38050 38090 38115 38310 38550 38575 38765 38840 39120 39145 39210 39290 39310 39430 39710 40005 40010 40125 40210 40510 41005 41085 41410 41510 41615 42005 42010 42125 42155 42200 42260 42480 43010 43115 43515 44010 44205 44410 44510 44600 44710 44895 45005 45130 46010 46090 46400 46415 46810 47010 47210 47215 47615 47920 48005 48010 48050 48750 48950 49510 49515 49715 50005 50010 50210 50510 50515 50575 50770 51010 51515 51770 52740 53010 53015 53135 54010 54030 56170 57210 57240 58010 58215 58720 59510 60005 60610 61005 61045 62005 62010 62025 62510 62680 65005 65010 65510 65515 65810 66460 66600 69010 70010 71010 72710 73010 74410 75000 999999\n",
      "num: 3.75\n",
      "nom: 0 63 144 253\n",
      "num: 83.0\n",
      "nom: 0 59 63\n",
      "num: 3.125\n",
      "nom: nominal high very low low\n",
      "num: 84.0020060180542\n",
      "nom: 0 1 1 5 2 2 5 2 7 3 4 5 6 9 10 14\n",
      "num: 86.0\n",
      "nom:  09900 00020 00015 00013 00008 00006 00002 0 50 250 450 550 950 1050 3650 7950 10050 999999\n",
      "num: 78.0\n",
      "nom: 0 3 27 43 252 253\n",
      "num: 3.5\n",
      "nom: space ga\n",
      "num: 16.5\n",
      "nom: 0 1\n",
      "num: 81.0\n",
      "nom: 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "num: 84.375\n",
      "nom: 0 25 0 33 0 5 0 67 0 75 0 8 0 88 1 1 13 1 25 1 33 1 5\n",
      "num: 84.0\n",
      "nom: n t f\n",
      "num: 17.5\n",
      "nom: 1 9\n",
      "num: 87.0\n",
      "nom: 0 11 45 74 83 141\n",
      "num: 87.5\n",
      "nom: kdd coil 7\n",
      "num: 88.0\n",
      "nom: plasma retinol\n",
      "num: 89.0\n",
      "nom:  0 1 1 1 1 2 1 4 1 7 1 9 10 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 11 11 1 11 2 11 3 11 4 11 5 11 6 11 7 11 8 11 9 12 12 1 12 2 12 3 12 4 12 5 12 6 12 7 12 8 12 9 13 13 1 13 2 13 3 13 4 13 5 13 6 13 7 13 8 13 9 14 14 1 14 2 14 3 14 4 14 5 14 6 14 7 14 8 14 9 15 15 1 15 2 15 3 15 4 15 5 15 6 15 7 15 8 15 9 16 1 16 2 16 3 16 4 16 5 16 6 16 7 16 8 16 9 17 17 1 17 2 17 3 17 4 17 5 17 6 17 7 17 8 17 9 18 18 1 18 2 18 3 18 4 18 5 18 6 18 7 18 8 18 9 19 19 1 19 2 19 3 19 4 19 5 19 6 19 7 19 8 19 9 2 1 2 2 2 4 2 6 2 7 2 8 20 20 1 20 2 20 3 20 4 20 5 20 6 20 7 20 8 20 9 21 21 1 21 2 21 3 21 4 21 5 21 6 21 7 21 8 21 9 22 22 1 22 2 22 3 22 4 22 5 22 6 22 7 22 8 22 9 23 23 1 23 2 23 3 23 4 23 5 23 6 23 7 23 8 23 9 24 24 1 24 2 24 3 24 4 24 5 24 6 24 7 24 8 24 9 25 25 1 25 2 25 3 25 4 25 5 25 6 25 7 25 8 25 9 26 26 1 26 2 26 3 26 4 26 5 26 6 26 7 26 8 26 9 27 27 1 27 2 27 3 27 4 27 5 27 6 27 7 27 8 27 9 28 28 1 28 2 28 3 28 4 28 5 28 6 28 7 28 8 28 9 29 29 1 29 2 29 3 29 4 29 5 29 6 29 7 29 8 29 9 3 3 1 3 2 3 7 3 8 3 9 30 30 1 30 2 30 3 30 4 30 5 30 6 30 7 30 8 30 9 31 31 1 31 2 31 3 31 4 31 5 31 6 31 7 31 8 31 9 32 32 1 32 2 32 3 32 4 32 5 32 6 32 9 33 33 2 33 3 33 6 33 7 34 3 34 7 34 9 35 3 35 7 36 1 4 2 4 3 4 4 4 6 4 7 4 9 5 2 5 3 5 5 5 6 5 7 5 8 6 6 1 6 3 6 4 6 5 6 6 6 7 6 8 6 9 7 7 1 7 2 7 3 7 4 7 5 7 6 7 7 7 8 7 9 8 8 1 8 2 8 3 8 4 8 5 8 6 8 7 8 8 8 9 9 1 9 2 9 3 9 4 9 5 9 6 9 7 9 8 9 9\n",
      "num: 98.61751152073731\n",
      "nom: 0 6 53 60 67 85 94 162 190\n",
      "num: 98.7012987012987\n",
      "nom: blood transfusion service center\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(\"num: \"+ str(vec_num[i]))\n",
    "    print(\"nom: \"+ vec_nom[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_vectors(vec1,vec2):\n",
    "    return (1- cosine(vec1,vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#pip install fasttext\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('../models/cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttex_vectors(nodes_literals):\n",
    "    nodes_vector = []\n",
    "    for literal in nodes_literals:\n",
    "        \n",
    "        if is_number(literal):\n",
    "            literal = str(literal)\n",
    "        \n",
    "        nodes_vector.append(ft.get_sentence_vector(literal))\n",
    "        \n",
    "    return nodes_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "#load model in memory\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_vectors(nodes_literals):\n",
    "    nodes_vector = []\n",
    "    for literal in nodes_literals:\n",
    "        \n",
    "        if is_number(literal):\n",
    "            literal = str(literal)\n",
    "        \n",
    "        #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "        tokenized = tokenizer.encode(literal, add_special_tokens=True,max_length=512)\n",
    "        input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        #result shape: (batch size, sequence length, model hidden dimension)\n",
    "        #print(last_hidden_states.shape)\n",
    "        \n",
    "        #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "        nodes_vector.append(torch.mean(last_hidden_states[0],dim=0).detach()) \n",
    "    return nodes_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings of words and calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext dif: 0.520315945148468\n",
      "Bert dif: 0.7249993681907654\n"
     ]
    }
   ],
   "source": [
    "s1= \"University degree\"\n",
    "s2=\"Bachelor\"\n",
    "vectors_b = bert_vectors([s1,s2])\n",
    "vectors_ft = fasttex_vectors([s1,s2])\n",
    "print(\"Fasttext dif: \" + str(cosine_vectors(vectors_ft[0],vectors_ft[1])))\n",
    "print(\"Bert dif: \" + str(cosine_vectors(vectors_b[0],vectors_b[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.8800E+10\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "rep = '%.4E' % Decimal(88799777378)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectors, columns=[\"colummn\"])\n",
    "df.to_csv('lit_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lit_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
