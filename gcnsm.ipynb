{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy\n",
    "#pip install pandas\n",
    "#pip install pandas-profiling\n",
    "#pip install networkx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path)\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n",
    "\n",
    "def plot_graph(g,ds_nodes=[],attribute_nodes=[],feat_nodes=[],lit_nodes=[]):\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=ds_nodes,node_color=\"blue\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=attribute_nodes,node_color=\"green\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=feat_nodes,node_color=\"grey\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=lit_nodes,node_color=\"red\",node_size=900)\n",
    "\n",
    "    nx.draw_networkx_edges(g,pos,width=3)\n",
    "    nx.draw_networkx_labels(g,pos,font_size=8)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph  construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_ds_id(data):\n",
    "    return \"DS_\"+data\n",
    "def code_attr_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_feat_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_literal_id(data,parent):\n",
    "    return \"literal_\"+data+\"|\"+parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset(datasets,g=None,instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=bert(\"dataset\"),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=bert(\"feature dataset\"),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=bert(row[i]),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_attribute(datasets,g=None,instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=bert(\"attribute\"),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=bert(\"feature attribute\"),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=bert(row[i]),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = read_dataset(\"./openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset(df_dataset,g,0)\n",
    "df_attributes = read_dataset(\"./openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes,g,0)\n",
    "df_attributes_numeric = read_dataset(\"./openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes_numeric,g,0)\n",
    "#plot_graph(g,ds_nodes,attr_nodes,feat_nodes,lit_nodes);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and read graph in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(g, \"encoded_features.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_gpickle(\"encoded_features.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get array of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(datasets_features))\n",
    "print(len(datasets_literals))\n",
    "print(len(attributes))\n",
    "print(len(attributes_features))\n",
    "print(len(attributes_literals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get nodes and print data from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6158e-01,  4.4368e-01, -1.7006e-01, -1.9077e-01,  1.5807e-01,\n",
      "        -2.6305e-02, -6.4893e-02,  2.8374e-01,  3.2929e-01,  5.7418e-01,\n",
      "         2.8632e-01,  1.7504e-02,  3.3841e-01, -1.9123e-01, -3.9816e-01,\n",
      "        -1.0002e+00,  9.1957e-02,  5.1864e-01,  3.5536e-01,  2.9942e-01,\n",
      "         6.1362e-01, -2.6814e-01,  3.7264e-01, -2.9228e-02,  4.1818e-01,\n",
      "        -2.7104e-01,  1.6840e-01, -3.3542e-01, -2.1305e-01,  4.5572e-01,\n",
      "         5.5157e-02, -2.0026e-01, -4.1137e-02,  3.9757e-02,  9.4895e-02,\n",
      "         8.2463e-02, -3.9485e-01,  4.0088e-01,  3.0982e-01, -2.6520e-01,\n",
      "        -3.5821e-01, -3.1978e-01, -2.1658e-01,  1.2864e-01,  5.1229e-01,\n",
      "        -1.7002e-01,  2.2455e-01,  3.5818e-02, -6.6891e-01,  2.9675e-02,\n",
      "        -6.4308e-01,  1.8412e-01, -2.8592e-01,  1.7483e-01, -5.0626e-01,\n",
      "        -4.2068e-01, -1.4417e-02,  5.5719e-02, -2.9205e-01,  5.3836e-01,\n",
      "         1.0070e-01, -2.3647e-01, -3.4717e-02,  4.2593e-01,  4.9275e-01,\n",
      "         9.7349e-02, -2.4548e-01, -1.7963e-01,  4.1090e-01, -4.6524e-01,\n",
      "         2.7759e-01, -6.7316e-01, -5.1720e-01, -1.2571e-02,  4.7416e-01,\n",
      "         5.8261e-02, -6.6805e-01, -3.5784e-01,  3.3626e-02,  2.3410e-01,\n",
      "         3.7865e-01, -2.2627e-01,  1.2057e-01, -5.4328e-01,  3.0009e-01,\n",
      "         4.3534e-01, -3.3075e-02, -1.0749e-02,  1.1022e-01, -1.2101e+00,\n",
      "         2.5327e-01, -3.4966e-01,  8.6200e-01, -4.8261e-01,  3.1677e-01,\n",
      "         1.5673e-01, -2.8282e-01,  8.5058e-01,  5.8379e-01,  1.5079e-01,\n",
      "         3.3168e-01,  1.6389e-01, -6.6324e-01, -2.6731e-01, -1.2490e-01,\n",
      "        -2.2702e-01,  4.2397e-01, -1.9515e-01, -1.4688e-01, -2.3710e-01,\n",
      "        -6.9065e-01, -1.6547e-01, -2.5573e-01, -9.1738e-01,  4.9339e-01,\n",
      "         3.9725e-01, -7.9235e-02,  1.3836e-01, -3.5703e-01,  2.6585e-01,\n",
      "         3.4573e-01,  3.1614e-02,  8.1422e-02,  2.5741e-01,  1.0330e-01,\n",
      "         8.4841e-02, -8.1440e-02,  3.3416e-01,  2.3357e-02,  1.0073e-01,\n",
      "        -2.6447e-01, -4.3778e-01, -1.9742e-01, -3.1799e-01,  2.1010e-01,\n",
      "         2.0210e-01,  3.9373e-01,  6.6137e-02,  1.8533e-02,  3.5085e-01,\n",
      "         3.8722e-01,  2.6054e-01, -3.9587e-02,  2.0273e-01,  2.8005e-01,\n",
      "        -2.3183e-01, -3.2452e-01,  1.5109e-01,  3.6968e-01,  1.2610e-01,\n",
      "        -5.2676e-01, -5.3323e-02,  2.6397e-01,  3.6080e-01, -4.2888e-01,\n",
      "         3.0369e-02,  8.5748e-01, -2.2753e-01,  4.8195e-01,  1.5392e-01,\n",
      "         3.6954e-01,  1.7941e-01,  1.3034e-01,  3.2715e-01, -1.2640e-01,\n",
      "         2.6511e-01, -1.9108e-01,  3.0196e-01, -1.5140e-01, -7.5628e-02,\n",
      "         5.8367e-01,  3.4406e-01,  4.7054e-02, -3.1622e-01, -2.9750e-01,\n",
      "        -6.5828e-02, -2.7785e-01,  1.8218e-01,  5.7409e-02, -2.2552e-01,\n",
      "        -2.0000e-02,  2.7233e-01,  2.8755e-01, -3.7211e-01,  1.3933e-01,\n",
      "        -1.6027e-01,  1.1358e-01,  1.0293e-01,  2.6702e-01,  8.0337e-01,\n",
      "        -1.5725e-01,  4.9985e-01,  2.6801e-01,  6.4094e-01,  2.5982e-01,\n",
      "        -1.9223e-02,  2.0380e-01,  2.2679e-01, -7.3601e-02,  3.8304e-01,\n",
      "        -2.5727e-01, -1.0566e-01, -1.5131e-01,  3.5556e-01, -8.1573e-02,\n",
      "        -1.9216e-01,  6.1712e-01, -3.2754e-01, -1.9007e-01, -2.4430e-01,\n",
      "         1.2843e-02, -1.8969e-01,  1.0421e-01, -9.6121e-02,  7.0665e-01,\n",
      "         1.0668e-02,  8.5451e-02,  8.9284e-01,  9.6687e-02,  3.4131e-02,\n",
      "         2.7388e-01,  5.0242e-01, -4.2168e-01,  3.0188e-01,  3.8132e-01,\n",
      "         3.4764e-01, -1.0496e-01, -2.6058e-01, -2.1931e-01, -2.9296e-01,\n",
      "        -5.6384e-01, -8.3199e-01,  2.8852e-01, -1.6097e-01, -1.8562e-01,\n",
      "        -1.5416e-01, -1.0857e-01,  2.3081e-01,  4.9055e-01, -3.3222e-01,\n",
      "        -1.8320e-02,  2.9903e-01, -4.4695e-01,  3.2374e-01,  1.7955e-01,\n",
      "        -4.9481e-01,  2.6918e-01, -8.1238e-02,  1.7203e-01,  1.0686e-01,\n",
      "         1.7335e-01, -8.8099e-02, -1.7092e-01,  3.1375e-01, -4.8432e-01,\n",
      "         7.5798e-01, -2.3567e-01, -7.0480e-01,  8.3257e-04, -1.0521e-01,\n",
      "        -6.3462e-01,  6.0065e-02,  2.1037e-03, -3.9037e-01, -4.3370e-01,\n",
      "        -8.3563e-02, -2.3319e-01, -8.7173e-02, -7.5822e-01, -1.2363e-01,\n",
      "         1.0261e-01, -5.5676e-01, -8.7347e-02, -5.6531e-01, -6.0554e-01,\n",
      "        -8.2277e-03, -1.9565e-01, -4.1638e-01,  1.6730e-02,  1.8965e-01,\n",
      "         4.5277e-02,  3.3999e-01,  1.9363e-01, -4.8213e-01,  3.9377e-01,\n",
      "         3.2646e-01, -5.2690e-01, -1.5549e-01,  3.7059e-01, -5.3761e-01,\n",
      "         3.0556e-01,  1.9909e-01, -8.6356e-01,  1.8216e-01,  1.0842e-01,\n",
      "         6.5089e-02, -3.0779e-01,  3.6467e-01,  4.6530e-02,  2.3104e-01,\n",
      "        -2.3229e-01,  1.9965e-01, -4.4831e-01, -2.1563e-01,  2.8862e-02,\n",
      "        -2.8744e-01,  2.2596e-01,  4.2557e-01, -4.8302e-01,  1.3064e-01,\n",
      "        -6.1407e-03, -1.0907e-01,  1.7967e-01,  3.4057e-01, -7.9926e-01,\n",
      "         3.9148e-04, -3.7253e-01, -1.9707e-01,  7.5669e-02, -4.4260e-01,\n",
      "         3.9590e-01, -5.0605e-01, -1.5125e-01, -1.3086e-01, -1.1669e-01,\n",
      "        -7.1311e-01,  1.3941e-01,  4.3250e-01,  4.1371e-01,  3.4727e-02,\n",
      "         7.2788e-01, -1.1295e-01, -4.2348e-02, -3.0420e-01,  1.6504e-01,\n",
      "         4.7877e-01, -2.3499e-01, -4.0324e-02,  3.0380e-01,  5.2618e-01,\n",
      "         4.5889e-01, -1.1700e-01,  3.1604e-01, -2.5130e-01, -8.0599e-02,\n",
      "         1.2783e-01,  3.1229e-01,  8.4158e-02,  1.7666e-01,  1.2861e+00,\n",
      "        -1.6854e-01, -2.7517e-01, -1.9898e-01,  4.6113e-01, -5.1684e-01,\n",
      "         5.4069e-01,  3.4314e-01,  1.7605e-01, -1.1590e-01, -4.5133e-01,\n",
      "        -2.8020e-01,  5.7800e-02, -1.7081e-01, -8.8861e-01, -2.1448e-01,\n",
      "         1.6337e-01,  4.9628e-02, -8.5878e-01, -3.5818e-01, -4.3742e-01,\n",
      "        -1.0995e-01, -1.8159e-01, -2.4118e-01, -7.4674e-01, -1.3691e-01,\n",
      "        -1.7379e-01,  2.3266e-01,  4.6264e-01,  2.6305e-01, -2.9588e-02,\n",
      "        -3.5598e-01, -3.5453e-01, -4.4760e-01,  4.2968e-02,  1.2401e-01,\n",
      "        -1.0320e-01, -5.5825e-01, -2.7718e-01,  3.6125e-01, -2.2121e-03,\n",
      "         3.5144e-01,  3.1634e-01, -2.1904e-01,  9.0413e-02,  1.6923e-01,\n",
      "        -9.5963e-03,  1.3184e-01, -1.6343e-01, -4.3719e-01,  1.8828e-01,\n",
      "         4.6575e-01,  3.8197e-01, -3.0759e-01,  2.3822e-01,  1.2839e-01,\n",
      "         2.6444e-01, -9.5402e-02,  7.0318e-02,  4.2061e-01,  3.0223e-02,\n",
      "         5.3272e-01, -4.7744e-01,  4.2506e-01, -4.6061e-01, -3.5235e-01,\n",
      "         2.0586e-01,  2.5320e-01, -2.3236e-01, -6.1352e-01,  6.2403e-01,\n",
      "        -4.7153e-01,  7.1405e-01, -7.0839e-02, -5.0115e-01,  8.6562e+00,\n",
      "        -1.1122e+00, -6.3999e-02,  5.0297e-01, -2.9353e-02,  2.3930e-01,\n",
      "        -1.9858e-01,  2.9951e-01, -1.1435e-01,  4.1388e-01, -2.0686e-01,\n",
      "        -2.9480e-01, -3.4361e-01,  4.2653e-01,  3.0224e-01,  3.1340e-01,\n",
      "        -6.1938e-01,  3.4690e-01, -3.5459e-01, -2.4089e-01,  5.4664e-01,\n",
      "        -3.0467e-01,  5.0296e-01,  3.0980e-01, -4.0188e-01, -1.2417e-01,\n",
      "         1.0040e-01, -1.6700e-01, -1.7238e-01, -1.0012e-02, -1.7299e-01,\n",
      "         3.7874e-01,  6.8630e-02,  1.3846e-01,  4.6870e-03,  3.0570e-01,\n",
      "        -2.6829e-01,  6.3431e-01,  2.6204e-01,  4.4796e-01, -2.1113e-01,\n",
      "        -2.3676e-01,  1.9878e-01, -2.1830e-01,  2.5316e-01,  2.2358e-01,\n",
      "         3.3361e-01, -3.8069e-01,  2.1045e-01, -4.1711e-01,  7.7747e-02,\n",
      "        -6.2540e-03, -2.6509e-02, -2.5444e-01,  4.7144e-01,  3.4510e-01,\n",
      "         4.9124e-01, -1.4576e-01,  9.5825e-02, -4.8394e-01, -2.8571e-01,\n",
      "        -1.4004e-02, -1.7444e-01, -5.7876e-01, -2.7916e-01,  5.9044e-02,\n",
      "         8.8753e-01,  1.1867e-01,  3.8367e-02, -3.2094e-01,  3.7690e-02,\n",
      "         7.4010e-01, -7.0537e-02,  1.1226e-01, -3.4344e-01, -4.0127e-01,\n",
      "         1.7526e-01, -5.2195e-01, -2.6848e-01,  2.5809e-01,  4.0014e-02,\n",
      "         5.5962e-02, -2.4857e-01, -4.1209e-01, -1.7675e-01,  3.4766e-01,\n",
      "         6.4201e-01, -8.7847e-01,  4.2429e-01, -5.8257e-01,  1.0635e-01,\n",
      "         4.1512e-01,  6.4167e-01,  4.4102e-01, -3.6328e-02, -3.5846e-01,\n",
      "         2.2355e-01, -3.2100e-01, -4.3662e-01,  1.5889e-02,  3.7312e-01,\n",
      "         2.1071e-03,  1.6607e-01, -1.4773e-01, -3.1619e-01, -6.4196e-01,\n",
      "        -1.4259e-01, -2.3467e-02,  3.3134e-01,  7.6169e-02,  1.7939e-01,\n",
      "         7.7033e-02, -4.6067e-03, -3.7926e-01,  4.6200e-01, -3.4214e-01,\n",
      "        -1.2482e-01,  5.2931e-01, -1.4231e-01,  1.5301e-01, -5.2527e-01,\n",
      "         3.4045e-01, -3.5356e-01,  1.3436e-01, -1.7511e-01,  8.0826e-03,\n",
      "        -5.8801e-01, -3.6292e-01,  1.2685e-01, -1.3058e-01,  4.3379e-01,\n",
      "        -3.1071e-01, -4.5559e-01, -3.7782e-01,  2.0943e-01, -1.5443e-01,\n",
      "         3.2262e-01, -1.3240e-01,  7.0712e-01,  2.1921e-01, -5.8845e-01,\n",
      "        -7.5791e-01, -6.8753e-01, -1.6382e-02,  2.4405e-01,  5.2059e-02,\n",
      "         3.5368e-01,  4.0708e-01, -2.4441e-01, -6.4736e-01, -4.0640e-02,\n",
      "         1.1723e-01,  3.0439e-01,  8.1801e-02,  2.9923e-01,  9.5984e-02,\n",
      "         1.0729e+00,  7.0078e-03,  8.5134e-01,  5.4701e-01, -3.6758e-02,\n",
      "         5.1551e-01,  1.0614e-01, -8.6384e-01, -8.7140e-01,  5.7160e-02,\n",
      "        -5.1301e-01,  6.4833e-01,  2.0020e-01,  3.2887e-01, -8.7440e-02,\n",
      "        -1.6820e-01, -2.3482e-01,  4.9198e-01,  4.9316e-01,  1.8919e-01,\n",
      "         7.9420e-02, -4.0541e-01, -3.9759e-02, -4.3499e-01,  1.9608e-01,\n",
      "         1.7036e-01,  1.8648e-01, -1.1035e-01, -3.8621e-01,  5.8651e-01,\n",
      "         1.4250e-02, -6.3140e-02,  3.3671e-01,  4.1268e-01, -2.5424e-01,\n",
      "         2.3326e-01, -5.6030e-03,  4.1623e-01,  6.3564e-01,  8.4073e-02,\n",
      "        -6.1503e-01,  3.1752e-01, -3.4743e-01, -4.8659e-01,  4.2695e-01,\n",
      "        -2.1549e-01,  7.2616e-01,  6.1021e-01, -5.9633e-01,  8.2791e-02,\n",
      "        -6.6746e-01,  4.7684e-01, -4.6223e-01,  9.4746e-01,  2.0355e-01,\n",
      "        -2.9152e-01,  5.0748e-01,  3.5356e-02, -1.7387e-01, -3.0896e-01,\n",
      "        -2.3216e-01, -3.2253e-01,  2.2533e-01, -2.5486e-01,  7.5100e-02,\n",
      "         1.9807e-01, -3.2237e-01,  5.2253e-01,  3.5243e-01, -6.0051e-02,\n",
      "        -5.8915e-01,  4.3575e-01,  1.4337e-01, -5.2305e-02,  9.5395e-02,\n",
      "        -1.4010e-01, -8.2745e-02,  3.6340e-01, -5.8175e-01,  3.0667e-01,\n",
      "        -6.4454e-02, -2.6541e-01, -3.2921e-01, -1.5142e-01,  1.1710e-01,\n",
      "         9.5442e-02,  2.6604e-01,  7.1503e-01, -8.5277e-01,  4.5802e-02,\n",
      "        -5.4356e-01, -1.5184e-01, -7.7679e-02,  8.8005e-01, -2.4295e-01,\n",
      "        -1.7308e-01,  1.3783e-02, -3.8539e-01,  1.1504e-01, -6.9806e-02,\n",
      "         4.9009e-02, -5.1690e-02,  2.3776e-01, -1.8164e-01,  5.7697e-02,\n",
      "         6.3463e-01, -8.8984e-01,  5.9636e-01,  7.7978e-01,  1.7219e-01,\n",
      "         2.1353e-01,  8.3753e-01, -3.3924e-01,  8.1356e-02, -2.6694e-01,\n",
      "         1.7988e-01, -4.7712e-01, -1.2942e-01, -2.4771e-01, -5.9124e-02,\n",
      "         1.2602e-01, -6.4918e-01, -4.3433e-01,  2.9506e-01,  1.9971e-01,\n",
      "         1.3011e-01,  6.2517e-02,  4.1497e-01, -4.1462e-01,  4.7923e-01,\n",
      "         5.1229e-02,  1.0811e-01, -7.6251e-02,  2.0286e-01,  1.4591e-02,\n",
      "         1.9951e-01,  1.0945e-01, -6.5328e-01, -1.1002e-01, -8.3751e-02,\n",
      "        -7.3293e-02,  1.2067e-02,  9.8481e-01, -8.4058e-02, -3.0368e-01,\n",
      "        -1.7630e-01, -2.2699e-01, -1.5144e-01, -5.6398e-02,  3.4912e-01,\n",
      "         2.7596e-01,  2.7626e-01,  7.9670e-02, -2.4268e-01,  1.5682e-01,\n",
      "         3.3836e-01,  5.8390e-02,  1.9032e-01,  2.3398e-01, -4.2443e-01,\n",
      "         1.3098e-01, -5.6858e-01, -4.8741e-01,  2.2899e-01, -1.9520e-01,\n",
      "         3.2709e-02,  3.8210e-01, -4.4161e-01, -2.4596e-01,  2.7187e-01,\n",
      "        -2.0809e-01, -1.8079e-01,  5.9017e-02,  3.2214e-01, -7.6726e-02,\n",
      "         8.7710e-02,  2.5305e-03,  8.2746e-02, -1.5986e-01,  1.4304e-01,\n",
      "         4.1553e-02, -5.1029e-04,  3.8306e-01, -5.9129e-01, -2.1445e-01,\n",
      "         2.2733e-01,  1.3604e-01,  5.2948e-01])\n"
     ]
    }
   ],
   "source": [
    "sample = [y for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "print(sample[0][\"vector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possitive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    possitive_pairs = []\n",
    "    negative_pairs = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[2] == 1:\n",
    "            possitive_pairs.append((row[0],row[1]))\n",
    "        else:\n",
    "            if randrange(1, 10) > 8:\n",
    "                negative_pairs.append((row[0],row[1]))\n",
    "    return possitive_pairs,negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching = read_dataset(\"./openml_203ds_datasets_matching.csv\",keep_columns=[\"'dataset1_id'\", \"'dataset2_id'\",\"'matching_topic'\"]);\n",
    "pos,neg = get_samples(df_matching)\n",
    "print(\"Possitive samples: \"+str(len(pos)) + \" Negative samples (10%): \"+str(len(neg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    \"\"\" Returns True is string is a number. \"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tokens(nodes):\n",
    "    #nodes_unique = set(nodes)\n",
    "    nodes_unique = nodes\n",
    "    numerical=[]\n",
    "    nominal=[]\n",
    "    for s in nodes_unique:\n",
    "        if is_number(s):\n",
    "            numerical.append(float(s))\n",
    "        else:\n",
    "            #nominal = nominal +  list(map(lambda a : re.sub(r'[^A-Za-z0-9 \\']+','',a).lower(),(re.split('[-_;|]\\s*',s))))\n",
    "            nominal.append(re.sub(r'[^A-Za-z0-9 \\']+',' ',s).lower())\n",
    "            #nominal = nominal +  s.split(\"|\")\n",
    "    return list(set(numerical)),list(set(nominal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_from_number(n):\n",
    "    #get the embeddings for numerical values that make sense\n",
    "    out = []\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_num,vec_nom = get_tokens(lit_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(\"num: \"+ str(vec_num[i]))\n",
    "    print(\"nom: \"+ vec_nom[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_vectors(vec1,vec2):\n",
    "    return (1- cosine(vec1,vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fasttext\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('../models/cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttex(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "\n",
    "    return ft.get_sentence_vector(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttex_vectors(nodes_literals):\n",
    "    nodes_vector = []\n",
    "    for literal in nodes_literals:\n",
    "        \n",
    "        if is_number(literal):\n",
    "            literal = str(literal)\n",
    "        \n",
    "        nodes_vector.append(ft.get_sentence_vector(literal))\n",
    "        \n",
    "    return nodes_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "#load model in memory\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "\n",
    "    #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "    tokenized = tokenizer.encode(value, add_special_tokens=True,max_length=512)\n",
    "    input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    #result shape: (batch size, sequence length, model hidden dimension)\n",
    "    #print(last_hidden_states.shape)\n",
    "\n",
    "    #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "    return torch.mean(last_hidden_states[0],dim=0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_vectors(nodes_literals):\n",
    "    nodes_vector = []\n",
    "    for literal in nodes_literals:\n",
    "        \n",
    "        if is_number(literal):\n",
    "            literal = str(literal)\n",
    "        \n",
    "        #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "        tokenized = tokenizer.encode(literal, add_special_tokens=True,max_length=512)\n",
    "        input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        #result shape: (batch size, sequence length, model hidden dimension)\n",
    "        #print(last_hidden_states.shape)\n",
    "        \n",
    "        #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "        nodes_vector.append(torch.mean(last_hidden_states[0],dim=0).detach()) \n",
    "    return nodes_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings of words and calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1= \"University degree\"\n",
    "s2=\"Bachelor\"\n",
    "vectors_b = bert_vectors([s1,s2])\n",
    "vectors_ft = fasttex_vectors([s1,s2])\n",
    "print(\"Fasttext dif: \" + str(cosine_vectors(vectors_ft[0],vectors_ft[1])))\n",
    "print(\"Bert dif: \" + str(cosine_vectors(vectors_b[0],vectors_b[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "rep = '%.4E' % Decimal(88799777378)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectors, columns=[\"colummn\"])\n",
    "df.to_csv('lit_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lit_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
