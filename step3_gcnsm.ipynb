{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset with ~80% train, ~20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1776 Test samples: 2535\n",
      "Train positive samples: 888 Test positive samples: 99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from step3 import step3_train_test_split as ds_split\n",
    "file_name = \"openml_203ds_datasets_matching.csv\"\n",
    "#ds_split.split_topic(file_name)\n",
    "train_mask = pd.read_csv(\"./datasets/\"+file_name+\"_train2x.csv\").to_numpy()\n",
    "test_mask = pd.read_csv(\"./datasets/\"+file_name+\"_test2x.csv\").to_numpy()\n",
    "\n",
    "#info about split\n",
    "train_positive = np.array([x for x in train_mask if x[2]==1])\n",
    "test_positive = np.array([x for x in test_mask if x[2]==1])\n",
    "print(\"Train samples: \"+str(len(train_mask)) + \" Test samples: \"+str(len(test_mask)))\n",
    "print(\"Train positive samples: \"+str(len(train_positive)) + \" Test positive samples: \"+str(len(test_positive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read graph of metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g_x = nx.read_gpickle(\"./word_embeddings/encoded_fasttext.gpickle\")\n",
    "#g_x = nx.read_gpickle(\"./word_embeddings/encoded_bert.gpickle\")\n",
    "ds_order = 0\n",
    "for x,n in sorted(g_x.nodes(data=True)):\n",
    "    t = n['tipo']\n",
    "    if t == \"dataset\":\n",
    "        n['tipo'] = 0\n",
    "    if t == \"feature dataset\":\n",
    "        n['tipo'] = 1\n",
    "    if t == \"literal dataset\":\n",
    "        n['tipo'] = 2\n",
    "    if t == \"attribute\":\n",
    "        n['tipo'] = 3\n",
    "    if t == \"feature attribute\":\n",
    "        n['tipo'] = 4\n",
    "    if t == \"literal attribute\":\n",
    "        n['tipo'] = 5  \n",
    "    n['ds_order']=ds_order\n",
    "    ds_order+=1\n",
    "    \n",
    "datasets = [x for (x,y) in g_x.nodes(data=True) if y['tipo']==0]\n",
    "ds_order = [y['ds_order'] for x,y in g_x.nodes(data=True) if y['tipo']==0]\n",
    "map_ds = dict(zip(datasets,ds_order))\n",
    "#map_reverse_ds_order = dict(zip(ds_order,datasets))\n",
    "map_ds['DS_1']\n",
    "\n",
    "for mask in train_mask:\n",
    "    mask[0] = map_ds[\"DS_\"+str(mask[0])]\n",
    "    mask[1] = map_ds[\"DS_\"+str(mask[1])]\n",
    "    if mask[2] == 0:\n",
    "        mask[2] = -1\n",
    "for mask in test_mask:\n",
    "    mask[0] = map_ds[\"DS_\"+str(mask[0])]\n",
    "    mask[1] = map_ds[\"DS_\"+str(mask[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export graph to deep graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "#convert from networkx to graph deep library format\n",
    "g = dgl.DGLGraph()\n",
    "g.from_networkx(g_x,node_attrs=['tipo','vector','ds_order'], edge_attrs=None)\n",
    "g_x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy based on thresholds of distance (e.g. cosine > 0.8 should be a positive pair)\n",
    "def threshold_acc(model, g, features, mask,loss):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    \n",
    "    mask = np.array([x for x in mask if x[2]==1])\n",
    "    \n",
    "    z1, z2 = model(g,features,mask[:,0],mask[:,1])\n",
    "    \n",
    "    #dist() | m - dist()\n",
    "    if loss == \"ContrastiveLoss\" or loss == \"Euclidean\":\n",
    "        pdist = th.nn.PairwiseDistance(p=2)        \n",
    "        result = pdist(z1,z2)\n",
    "        for i in range(len(result)):\n",
    "            r = result[i]\n",
    "            if r.item() <= 0.2:\n",
    "                indices.append(1.0)\n",
    "            else:\n",
    "                indices.append(0.0)          \n",
    "        indices_tensor = th.tensor(indices)\n",
    "        labels_tensor = th.tensor(mask[:,2])\n",
    "        \n",
    "    #1 - cos() | max(0,cos() - m)\n",
    "    if loss == \"CosineEmbeddingLoss\":\n",
    "        cos = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        result = cos(z1,z2)\n",
    "        for i in range(len(result)):\n",
    "            r = result[i]\n",
    "            if r.item() >= 0.8:\n",
    "                indices.append(1.0)\n",
    "            else:\n",
    "                indices.append(0.0)\n",
    "        indices_tensor = th.tensor(indices)\n",
    "        labels_tensor = th.tensor(mask[:,2])\n",
    "    \n",
    "    correct = th.sum(indices_tensor == labels_tensor)\n",
    "    return correct.item() * 1.0 / len(labels_tensor)\n",
    "\n",
    "# Accuracy based on nearest neighboor (e.g. the nearest node should be a positive pair)\n",
    "def ne_ne_acc(model, g, features, mask,loss):\n",
    "\n",
    "    mask_concat = np.concatenate((mask[:,0],mask[:,1]))\n",
    "    mask_indices = np.unique(mask_concat)\n",
    "    \n",
    "    mask_pos_samples = np.array([x for x in mask if x[2]==1])    \n",
    "    mask_pos_samples_concat = np.concatenate((mask_pos_samples[:,0],mask_pos_samples[:,1]))\n",
    "    mask_pos_samples_indices = np.unique(mask_pos_samples_concat)\n",
    "    \n",
    "    mask_embeddings,mask_pos_samples_embeddings = model(g, features,mask_indices,mask_pos_samples_indices)\n",
    "    \n",
    "    sum_accuracy = 0\n",
    "    for i in range(len(mask_pos_samples_indices)):\n",
    "        candidate = mask_pos_samples_embeddings[i]\n",
    "        #dist() | m - dist()\n",
    "        if loss == \"ContrastiveLoss\":\n",
    "            pdist = th.nn.PairwiseDistance(p=2)        \n",
    "            result = pdist(candidate,mask_embeddings)\n",
    "            largest = False\n",
    "        #1 - cos() | max(0,cos() - m)\n",
    "        if loss == \"CosineEmbeddingLoss\":\n",
    "            thecos = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            result = thecos(candidate.reshape(1,len(candidate)),mask_embeddings)\n",
    "            largest = True\n",
    "        \n",
    "        #we ignore the result of the vector with itself\n",
    "        result_index = th.topk(result, 2, largest=largest).indices[-1]\n",
    "        closest_node_index = mask_indices[result_index]\n",
    "        \n",
    "#         check_relation_nodes = np.array([x for x in pos_samples \n",
    "        check_relation_nodes = np.array([x for x in mask_pos_samples \n",
    "                                         if (x[0]==mask_pos_samples_indices[i] and x[1]==closest_node_index) or \n",
    "                                         (x[1]==mask_pos_samples_indices[i] and x[0]==closest_node_index)])\n",
    "        \n",
    "        if len(check_relation_nodes) > 0:\n",
    "            sum_accuracy += 1\n",
    "\n",
    "    return sum_accuracy / len(mask_pos_samples_indices)    \n",
    "\n",
    "\n",
    "def evaluate(model, g, features, mask,loss):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        #naive way of testing accuracy \n",
    "        acc = threshold_acc(model, g, features, mask,loss)\n",
    "        #accuracy based on 1-NN \n",
    "        acc2 = ne_ne_acc(model, g, features, mask,loss)\n",
    "        return acc,acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(training.net,g,g.ndata['vector'],test_mask,training.loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "def train(training,iterations):\n",
    "    dur = []\n",
    "    \n",
    "    ## training.splits indicates number of sets to split, not batch size!\n",
    "    train_batch = np.array_split(train_mask,training.batch_splits)\n",
    "    \n",
    "    #specify number of threads for the training\n",
    "    #th.set_num_threads(2)\n",
    "    \n",
    "    for epoch in range(iterations):\n",
    "        #model train mode\n",
    "        training.net.train()\n",
    "        t0 = time.time()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        #forward_backward positive batch sample\n",
    "        for split in train_batch:\n",
    "            z1,z2 = training.net(g, g.ndata['vector'],split[:,0],split[:,1])\n",
    "            loss = training.loss(z1,z2, th.tensor(split[:,2]))\n",
    "            training.optimizer.zero_grad()\n",
    "            #loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            training.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = epoch_loss / training.batch_splits\n",
    "\n",
    "        #runtime\n",
    "        t = time.time() - t0\n",
    "        dur.append(t)\n",
    "        \n",
    "        #total time accumulation for this model\n",
    "        training.runtime_seconds+=t\n",
    "        \n",
    "        #accuracy\n",
    "        acc,acc2 = evaluate(training.net, g, g.ndata['vector'], test_mask,training.loss_name)\n",
    "        \n",
    "        #create log\n",
    "        output = {}\n",
    "        output['epoch'] = training.epochs_run\n",
    "        output['loss'] = float('%.5f'% (epoch_loss))\n",
    "        output['acc'] = float('%.5f'% (acc))\n",
    "        output['acc2'] = float('%.5f'% (acc2))\n",
    "        output['time_epoch'] = float('%.5f'% (np.mean(dur)))\n",
    "        output['time_total'] = float('%.5f'% (training.runtime_seconds))\n",
    "        training.log.append(output)\n",
    "        training.epochs_run+=1\n",
    "        print(str(output))\n",
    "        \n",
    "    #write results and save model to files\n",
    "    training.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and run training\n",
    "### NN architectures: \n",
    "{<br>\n",
    "    \"0\": \"Bert_300\", <br>\n",
    "    \"1\": \"Bert_300_300_200\", <br>\n",
    "    \"2\": \"Bert_768\", <br>\n",
    "    \"3\": \"Fasttext_150\", <br>\n",
    "    \"4\": \"Fasttext_150_150_100\", <br>\n",
    "    \"5\": \"Fasttext_300\" <br>\n",
    "}\n",
    "### Loss functions: \n",
    "{<br>\n",
    "    \"0\": \"ContrastiveLoss\", <br>\n",
    "    \"1\": \"CosineEmbeddingLoss\", <br>\n",
    "    \"2\": \"Euclidean\" <br>\n",
    "}\n",
    "### Example to define architecture and loss\n",
    "<b>from step3 import step3_gcn_nn_concatenate as gcn_nn</b> <br>\n",
    "<b>from step3 import step3_gcn_loss as gcn_loss</b> <br>\n",
    "print(gcn_nn.get_options()) #list of options<br>\n",
    "print(gcn_loss.get_options()) #list of options<br>\n",
    "\n",
    "### Load training class to save/load/train experiments:\n",
    "<b>from step3 import step3_gcn_train as gcn_train</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from step3 import step3_gcn_nn_concatenate as gcn_nn\n",
    "from step3 import step3_gcn_loss as gcn_loss\n",
    "from step3 import step3_gcn_training as gcn_training\n",
    "\n",
    "# #load model from path\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/[file_name].pt\")\n",
    "# train(training,iterations=N)\n",
    "\n",
    "# #train new model and specify parameters\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(),  #_of_option for NN architecture\n",
    "#             batch_splits= ,#_of_sets(this will (give dataset / batch_splits) size of batch\n",
    "#             lr= , #learning rate for training (e.g. 1e-3 )\n",
    "#             loss_name=gcn_loss.get_option_name() #_of_option for loss ,\n",
    "#             loss_parameters=) #loss function parameters separated by '+' e.g. for cosine and contrastive \"0.0+mean\" \n",
    "# train(training,iterations=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'loss': 0.14179, 'acc': 0.58586, 'acc2': 0.07143, 'time_epoch': 145.41325, 'time_total': 145.41325}\n",
      "{'epoch': 1, 'loss': 0.12487, 'acc': 0.49495, 'acc2': 0.07143, 'time_epoch': 149.09272, 'time_total': 298.18544}\n",
      "{'epoch': 2, 'loss': 0.12357, 'acc': 0.44444, 'acc2': 0.07143, 'time_epoch': 148.57282, 'time_total': 445.71846}\n",
      "{'epoch': 3, 'loss': 0.11989, 'acc': 0.38384, 'acc2': 0.07143, 'time_epoch': 149.43223, 'time_total': 597.72891}\n",
      "{'epoch': 4, 'loss': 0.11362, 'acc': 0.38384, 'acc2': 0.07143, 'time_epoch': 149.70886, 'time_total': 748.54428}\n",
      "{'epoch': 5, 'loss': 0.10828, 'acc': 0.42424, 'acc2': 0.08036, 'time_epoch': 149.64743, 'time_total': 897.88457}\n",
      "{'epoch': 6, 'loss': 0.10632, 'acc': 0.30303, 'acc2': 0.0625, 'time_epoch': 150.01371, 'time_total': 1050.09598}\n",
      "{'epoch': 7, 'loss': 0.10518, 'acc': 0.33333, 'acc2': 0.0625, 'time_epoch': 150.04749, 'time_total': 1200.37993}\n",
      "{'epoch': 8, 'loss': 0.10437, 'acc': 0.43434, 'acc2': 0.0625, 'time_epoch': 155.98512, 'time_total': 1403.8661}\n",
      "{'epoch': 9, 'loss': 0.10434, 'acc': 0.29293, 'acc2': 0.0625, 'time_epoch': 159.71926, 'time_total': 1597.19262}\n",
      "{'epoch': 10, 'loss': 0.09956, 'acc': 0.34343, 'acc2': 0.0625, 'time_epoch': 159.39694, 'time_total': 1753.36633}\n",
      "{'epoch': 11, 'loss': 0.09715, 'acc': 0.34343, 'acc2': 0.07143, 'time_epoch': 164.46614, 'time_total': 1973.59365}\n",
      "{'epoch': 12, 'loss': 0.09517, 'acc': 0.34343, 'acc2': 0.07143, 'time_epoch': 165.67352, 'time_total': 2153.75575}\n",
      "{'epoch': 13, 'loss': 0.09321, 'acc': 0.32323, 'acc2': 0.07143, 'time_epoch': 165.00903, 'time_total': 2310.12641}\n",
      "{'epoch': 14, 'loss': 0.08814, 'acc': 0.31313, 'acc2': 0.0625, 'time_epoch': 164.33344, 'time_total': 2465.00154}\n",
      "{'epoch': 15, 'loss': 0.08522, 'acc': 0.36364, 'acc2': 0.08036, 'time_epoch': 163.64334, 'time_total': 2618.29346}\n",
      "{'epoch': 16, 'loss': 0.08187, 'acc': 0.34343, 'acc2': 0.08929, 'time_epoch': 162.99171, 'time_total': 2770.85902}\n",
      "{'epoch': 17, 'loss': 0.07656, 'acc': 0.58586, 'acc2': 0.07143, 'time_epoch': 162.42794, 'time_total': 2923.7029}\n",
      "{'epoch': 18, 'loss': 0.09093, 'acc': 0.40404, 'acc2': 0.09821, 'time_epoch': 161.82232, 'time_total': 3074.62401}\n",
      "{'epoch': 19, 'loss': 0.08308, 'acc': 0.39394, 'acc2': 0.08929, 'time_epoch': 160.38012, 'time_total': 3207.60237}\n",
      "{'epoch': 20, 'loss': 0.08497, 'acc': 0.36364, 'acc2': 0.08036, 'time_epoch': 158.87991, 'time_total': 3336.47818}\n",
      "{'epoch': 21, 'loss': 0.07403, 'acc': 0.34343, 'acc2': 0.0625, 'time_epoch': 157.78458, 'time_total': 3471.26085}\n",
      "{'epoch': 22, 'loss': 0.07887, 'acc': 0.29293, 'acc2': 0.0625, 'time_epoch': 157.20732, 'time_total': 3615.76844}\n",
      "{'epoch': 23, 'loss': 0.07281, 'acc': 0.28283, 'acc2': 0.07143, 'time_epoch': 156.71464, 'time_total': 3761.15132}\n",
      "{'epoch': 24, 'loss': 0.0739, 'acc': 0.29293, 'acc2': 0.08036, 'time_epoch': 156.24182, 'time_total': 3906.04547}\n",
      "{'epoch': 25, 'loss': 0.06431, 'acc': 0.29293, 'acc2': 0.09821, 'time_epoch': 155.81117, 'time_total': 4051.09042}\n",
      "{'epoch': 26, 'loss': 0.06764, 'acc': 0.33333, 'acc2': 0.08929, 'time_epoch': 155.48974, 'time_total': 4198.22294}\n",
      "{'epoch': 27, 'loss': 0.06473, 'acc': 0.39394, 'acc2': 0.08036, 'time_epoch': 155.07674, 'time_total': 4342.14883}\n",
      "{'epoch': 28, 'loss': 0.06422, 'acc': 0.30303, 'acc2': 0.08036, 'time_epoch': 154.17513, 'time_total': 4471.07882}\n",
      "{'epoch': 29, 'loss': 0.06296, 'acc': 0.33333, 'acc2': 0.08036, 'time_epoch': 153.85706, 'time_total': 4615.71166}\n",
      "{'epoch': 30, 'loss': 0.05578, 'acc': 0.33333, 'acc2': 0.09821, 'time_epoch': 153.7081, 'time_total': 4764.95116}\n",
      "{'epoch': 31, 'loss': 0.05197, 'acc': 0.33333, 'acc2': 0.08929, 'time_epoch': 153.58674, 'time_total': 4914.77563}\n",
      "{'epoch': 32, 'loss': 0.054, 'acc': 0.33333, 'acc2': 0.08929, 'time_epoch': 153.44907, 'time_total': 5063.81931}\n",
      "{'epoch': 33, 'loss': 0.04969, 'acc': 0.33333, 'acc2': 0.07143, 'time_epoch': 153.32738, 'time_total': 5213.1309}\n",
      "{'epoch': 34, 'loss': 0.04876, 'acc': 0.33333, 'acc2': 0.09821, 'time_epoch': 153.24541, 'time_total': 5363.58939}\n",
      "{'epoch': 35, 'loss': 0.04478, 'acc': 0.34343, 'acc2': 0.08036, 'time_epoch': 152.77899, 'time_total': 5500.04356}\n",
      "{'epoch': 36, 'loss': 0.04361, 'acc': 0.35354, 'acc2': 0.08929, 'time_epoch': 152.58757, 'time_total': 5645.74006}\n",
      "{'epoch': 37, 'loss': 0.04412, 'acc': 0.40404, 'acc2': 0.10714, 'time_epoch': 152.54657, 'time_total': 5796.76963}\n",
      "{'epoch': 38, 'loss': 0.05154, 'acc': 0.33333, 'acc2': 0.10714, 'time_epoch': 152.52109, 'time_total': 5948.32269}\n",
      "{'epoch': 39, 'loss': 0.04558, 'acc': 0.34343, 'acc2': 0.08929, 'time_epoch': 152.5071, 'time_total': 6100.28381}\n",
      "{'epoch': 40, 'loss': 0.04371, 'acc': 0.41414, 'acc2': 0.08036, 'time_epoch': 152.48975, 'time_total': 6252.07992}\n",
      "{'epoch': 41, 'loss': 0.03883, 'acc': 0.39394, 'acc2': 0.08036, 'time_epoch': 152.37141, 'time_total': 6399.59928}\n",
      "{'epoch': 42, 'loss': 0.03799, 'acc': 0.55556, 'acc2': 0.07143, 'time_epoch': 151.89842, 'time_total': 6531.63199}\n",
      "{'epoch': 43, 'loss': 0.03568, 'acc': 0.45455, 'acc2': 0.08036, 'time_epoch': 151.39095, 'time_total': 6661.20197}\n",
      "{'epoch': 44, 'loss': 0.03437, 'acc': 0.41414, 'acc2': 0.08036, 'time_epoch': 150.87448, 'time_total': 6789.35173}\n",
      "{'epoch': 45, 'loss': 0.03947, 'acc': 0.47475, 'acc2': 0.08036, 'time_epoch': 150.36464, 'time_total': 6916.77356}\n",
      "{'epoch': 46, 'loss': 0.03981, 'acc': 0.38384, 'acc2': 0.0625, 'time_epoch': 149.88515, 'time_total': 7044.60195}\n",
      "{'epoch': 47, 'loss': 0.03091, 'acc': 0.39394, 'acc2': 0.08929, 'time_epoch': 149.40779, 'time_total': 7171.57408}\n",
      "{'epoch': 48, 'loss': 0.03469, 'acc': 0.42424, 'acc2': 0.07143, 'time_epoch': 148.94893, 'time_total': 7298.49764}\n",
      "{'epoch': 49, 'loss': 0.03119, 'acc': 0.40404, 'acc2': 0.08929, 'time_epoch': 148.5091, 'time_total': 7425.4552}\n",
      "{'epoch': 50, 'loss': 0.03417, 'acc': 0.48485, 'acc2': 0.08036, 'time_epoch': 148.08236, 'time_total': 7552.20021}\n",
      "{'epoch': 51, 'loss': 0.02967, 'acc': 0.45455, 'acc2': 0.07143, 'time_epoch': 147.66129, 'time_total': 7678.3871}\n",
      "{'epoch': 52, 'loss': 0.04039, 'acc': 0.45455, 'acc2': 0.08929, 'time_epoch': 147.24745, 'time_total': 7804.11506}\n",
      "{'epoch': 53, 'loss': 0.03378, 'acc': 0.66667, 'acc2': 0.05357, 'time_epoch': 146.8392, 'time_total': 7929.31696}\n",
      "{'epoch': 54, 'loss': 0.03946, 'acc': 0.46465, 'acc2': 0.0625, 'time_epoch': 146.4835, 'time_total': 8056.59223}\n",
      "{'epoch': 55, 'loss': 0.03129, 'acc': 0.41414, 'acc2': 0.08036, 'time_epoch': 146.13143, 'time_total': 8183.36031}\n",
      "{'epoch': 56, 'loss': 0.03203, 'acc': 0.50505, 'acc2': 0.08036, 'time_epoch': 145.77966, 'time_total': 8309.4409}\n",
      "{'epoch': 57, 'loss': 0.02754, 'acc': 0.47475, 'acc2': 0.08929, 'time_epoch': 145.44314, 'time_total': 8435.70206}\n",
      "{'epoch': 58, 'loss': 0.02508, 'acc': 0.51515, 'acc2': 0.08036, 'time_epoch': 145.1116, 'time_total': 8561.58443}\n",
      "{'epoch': 59, 'loss': 0.02148, 'acc': 0.50505, 'acc2': 0.05357, 'time_epoch': 144.79153, 'time_total': 8687.49172}\n",
      "Model and results saved\n",
      "{'epoch': 0, 'loss': 0.14335, 'acc': 0.90909, 'acc2': 0.07143, 'time_epoch': 129.9306, 'time_total': 129.9306}\n",
      "{'epoch': 1, 'loss': 0.13428, 'acc': 0.83838, 'acc2': 0.07143, 'time_epoch': 129.39347, 'time_total': 258.78693}\n",
      "{'epoch': 2, 'loss': 0.13132, 'acc': 0.83838, 'acc2': 0.08036, 'time_epoch': 128.91951, 'time_total': 386.75853}\n",
      "{'epoch': 3, 'loss': 0.1288, 'acc': 0.82828, 'acc2': 0.08929, 'time_epoch': 128.32877, 'time_total': 513.31508}\n",
      "{'epoch': 4, 'loss': 0.12842, 'acc': 0.80808, 'acc2': 0.09821, 'time_epoch': 127.89987, 'time_total': 639.49937}\n",
      "{'epoch': 5, 'loss': 0.12506, 'acc': 0.70707, 'acc2': 0.07143, 'time_epoch': 127.6092, 'time_total': 765.65523}\n",
      "{'epoch': 6, 'loss': 0.11952, 'acc': 0.63636, 'acc2': 0.07143, 'time_epoch': 127.47302, 'time_total': 892.31115}\n",
      "{'epoch': 7, 'loss': 0.11509, 'acc': 0.59596, 'acc2': 0.08036, 'time_epoch': 127.47851, 'time_total': 1019.82809}\n",
      "{'epoch': 8, 'loss': 0.11317, 'acc': 0.69697, 'acc2': 0.03571, 'time_epoch': 129.31247, 'time_total': 1163.8122}\n",
      "{'epoch': 9, 'loss': 0.11424, 'acc': 0.51515, 'acc2': 0.07143, 'time_epoch': 130.84362, 'time_total': 1308.43616}\n",
      "{'epoch': 10, 'loss': 0.1138, 'acc': 0.72727, 'acc2': 0.0625, 'time_epoch': 132.14872, 'time_total': 1453.63593}\n",
      "{'epoch': 11, 'loss': 0.11647, 'acc': 0.67677, 'acc2': 0.03571, 'time_epoch': 133.41338, 'time_total': 1600.96058}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 12, 'loss': 0.11032, 'acc': 0.57576, 'acc2': 0.05357, 'time_epoch': 134.4057, 'time_total': 1747.27405}\n",
      "{'epoch': 13, 'loss': 0.10647, 'acc': 0.56566, 'acc2': 0.05357, 'time_epoch': 135.21982, 'time_total': 1893.07744}\n",
      "{'epoch': 14, 'loss': 0.10538, 'acc': 0.67677, 'acc2': 0.0625, 'time_epoch': 136.0091, 'time_total': 2040.13647}\n",
      "{'epoch': 15, 'loss': 0.11275, 'acc': 0.66667, 'acc2': 0.0625, 'time_epoch': 136.90728, 'time_total': 2190.51647}\n",
      "{'epoch': 16, 'loss': 0.10687, 'acc': 0.67677, 'acc2': 0.07143, 'time_epoch': 137.55349, 'time_total': 2338.40926}\n",
      "{'epoch': 17, 'loss': 0.10655, 'acc': 0.65657, 'acc2': 0.07143, 'time_epoch': 138.19208, 'time_total': 2487.45747}\n",
      "{'epoch': 18, 'loss': 0.10432, 'acc': 0.78788, 'acc2': 0.0625, 'time_epoch': 137.91559, 'time_total': 2620.39624}\n",
      "{'epoch': 19, 'loss': 0.10072, 'acc': 0.52525, 'acc2': 0.0625, 'time_epoch': 137.48045, 'time_total': 2749.60907}\n",
      "{'epoch': 20, 'loss': 0.09725, 'acc': 0.66667, 'acc2': 0.07143, 'time_epoch': 137.0496, 'time_total': 2878.04153}\n",
      "{'epoch': 21, 'loss': 0.09925, 'acc': 0.63636, 'acc2': 0.05357, 'time_epoch': 136.68401, 'time_total': 3007.04817}\n",
      "{'epoch': 22, 'loss': 0.09848, 'acc': 0.50505, 'acc2': 0.07143, 'time_epoch': 136.29556, 'time_total': 3134.79781}\n",
      "{'epoch': 23, 'loss': 0.09445, 'acc': 0.50505, 'acc2': 0.07143, 'time_epoch': 135.94629, 'time_total': 3262.71092}\n",
      "{'epoch': 24, 'loss': 0.09587, 'acc': 0.55556, 'acc2': 0.07143, 'time_epoch': 135.81633, 'time_total': 3395.40836}\n",
      "{'epoch': 25, 'loss': 0.09738, 'acc': 0.67677, 'acc2': 0.07143, 'time_epoch': 136.23253, 'time_total': 3542.04576}\n",
      "{'epoch': 26, 'loss': 0.09809, 'acc': 0.60606, 'acc2': 0.07143, 'time_epoch': 136.79175, 'time_total': 3693.37726}\n",
      "{'epoch': 27, 'loss': 0.09457, 'acc': 0.56566, 'acc2': 0.07143, 'time_epoch': 137.16554, 'time_total': 3840.63502}\n",
      "{'epoch': 28, 'loss': 0.09314, 'acc': 0.53535, 'acc2': 0.05357, 'time_epoch': 137.42074, 'time_total': 3985.20142}\n",
      "{'epoch': 29, 'loss': 0.08882, 'acc': 0.51515, 'acc2': 0.07143, 'time_epoch': 137.66431, 'time_total': 4129.92919}\n"
     ]
    }
   ],
   "source": [
    "#Train with contrastive loss\n",
    "#train new model and specify parameters\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(3),\n",
    "#             batch_splits=14,\n",
    "#             lr=1e-2,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"1.0+mean\")\n",
    "# train(training,iterations=40)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# # training.load_state(path=\"./models/net_name:Fasttext_150|batch_splits:14.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.5+mean.pt\")\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(3),\n",
    "#             batch_splits=14,\n",
    "#             lr=1e-2,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.7+mean\")\n",
    "# train(training,iterations=40)\n",
    "\n",
    "#train new model and specify parameters\n",
    "training = gcn_training.Training()\n",
    "training.set_training(\n",
    "            net_name= gcn_nn.get_option_name(3),\n",
    "            batch_splits=14,\n",
    "            lr=1e-3,\n",
    "            loss_name=gcn_loss.get_option_name(0),\n",
    "            loss_parameters=\"0.7+mean\")\n",
    "train(training,iterations=60)\n",
    "\n",
    "# #Train with cosine loss\n",
    "training = gcn_training.Training()\n",
    "#train new model and specify parameters\n",
    "training.set_training(\n",
    "            net_name= gcn_nn.get_option_name(3),\n",
    "            batch_splits=14,\n",
    "            lr=1e-3,\n",
    "            loss_name=gcn_loss.get_option_name(1),\n",
    "            loss_parameters=\"0.7+mean\")\n",
    "train(training,iterations=60)\n",
    "\n",
    "#train new model and specify parameters\n",
    "training = gcn_training.Training()\n",
    "training.set_training(\n",
    "            net_name= gcn_nn.get_option_name(3),\n",
    "            batch_splits=14,\n",
    "            lr=1e-3,\n",
    "            loss_name=gcn_loss.get_option_name(1),\n",
    "            loss_parameters=\"0.3+mean\")\n",
    "train(training,iterations=60)\n",
    "\n",
    "#train new model and specify parameters\n",
    "training = gcn_training.Training()\n",
    "training.set_training(\n",
    "            net_name= gcn_nn.get_option_name(3),\n",
    "            batch_splits=14,\n",
    "            lr=1e-3,\n",
    "            loss_name=gcn_loss.get_option_name(1),\n",
    "            loss_parameters=\"0.5+mean\")\n",
    "train(training,iterations=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
