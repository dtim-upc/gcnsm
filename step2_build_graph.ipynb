{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset and plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy\n",
    "#pip install pandas\n",
    "#pip install pandas-profiling\n",
    "#pip install networkx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path)\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n",
    "\n",
    "def plot_graph(g,ds_nodes=[],attribute_nodes=[],feat_nodes=[],lit_nodes=[]):\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=ds_nodes,node_color=\"blue\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=attribute_nodes,node_color=\"green\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=feat_nodes,node_color=\"grey\",node_size=900)\n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=lit_nodes,node_color=\"red\",node_size=900)\n",
    "\n",
    "    nx.draw_networkx_edges(g,pos,width=3)\n",
    "    nx.draw_networkx_labels(g,pos,font_size=8)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph  construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_ds_id(data):\n",
    "    return \"DS_\"+data\n",
    "def code_attr_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_feat_id(data,parent):\n",
    "    return data+\"|\"+parent\n",
    "def code_literal_id(data,parent):\n",
    "    return \"literal_\"+data+\"|\"+parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset(datasets,g=None,instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "    \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[1:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = instances\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        g.add_node(dataset_id,vector=word_embedding(\"dataset\",\"fasttext\"),tipo=\"dataset\")\n",
    "        row = datasets.iloc[r][1:]\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min(instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_dataset_id = code_feat_id(features[i],dataset_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),dataset_id)\n",
    "            g.add_node(feature_dataset_id,vector=word_embedding(\"feature dataset\",\"fasttext\"),tipo=\"feature dataset\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],\"fasttext\"),tipo=\"literal dataset\")\n",
    "            g.add_edge(dataset_id,feature_dataset_id)\n",
    "            g.add_edge(feature_dataset_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_attribute(datasets,g=None,instances=0):\n",
    "    if g == None:\n",
    "        g = nx.Graph()\n",
    "        \n",
    "    #create nodes and edges at datasetLevel\n",
    "    features = datasets.columns[2:]\n",
    "    \n",
    "    if instances==0:\n",
    "        number_instances = len(datasets)\n",
    "    else:\n",
    "        number_instances = min (instances,len(datasets))\n",
    "    \n",
    "    for r in range(number_instances): \n",
    "        #node id is the openML id which is in the first column\n",
    "        #attr name is the 2nd column\n",
    "        dataset_id = code_ds_id(str(datasets.iloc[r][0]))\n",
    "        attribute_id = code_attr_id(datasets.iloc[r][1],dataset_id)\n",
    "        row = datasets.iloc[r][2:]\n",
    "        \n",
    "        g.add_node(attribute_id,vector=word_embedding(\"attribute\",\"fasttext\"),tipo=\"attribute\")\n",
    "        \n",
    "        #relation of dataset and an attribute\n",
    "        g.add_edge(dataset_id,attribute_id)\n",
    "        \n",
    "        if instances == 0:\n",
    "            number_features = len(features)\n",
    "        else:\n",
    "            number_features = min (instances,len(features))\n",
    "            \n",
    "        for i in range (number_features):\n",
    "            feature_attribute_id = code_feat_id(features[i],attribute_id)\n",
    "            literal_dataset_id = code_literal_id(str(i),attribute_id)\n",
    "            g.add_node(feature_attribute_id,vector=word_embedding(\"feature attribute\",\"fasttext\"),tipo=\"feature attribute\")\n",
    "            g.add_node(literal_dataset_id,vector=word_embedding(row[i],\"fasttext\"),tipo=\"literal attribute\")\n",
    "            g.add_edge(attribute_id,feature_attribute_id)\n",
    "            g.add_edge(feature_attribute_id,literal_dataset_id)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "df_dataset = read_dataset(\"./openML/openml_203ds_datasets_index.csv\",drop_columns=[\"Num\", \"dataset_topic\"]);\n",
    "g = g = nx.Graph()\n",
    "g = graph_dataset(df_dataset,g,1)\n",
    "df_attributes = read_dataset(\"./openML/openml_203ds_attributes_nominal_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes,g,2)\n",
    "df_attributes_numeric = read_dataset(\"./openML/openml_203ds_attributes_numeric_index.csv\",drop_columns=[\"dataset_name\", \"type_converted\"]);\n",
    "g = graph_attribute(df_attributes_numeric,g,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]\n",
    "plot_graph(g,datasets,attributes,datasets_features + attributes_features,datasets_literals + attributes_literals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and read graph in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write\n",
    "nx.write_gpickle(g, \"./word_embeddings/encoded_fasttext.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "g = nx.read_gpickle(\"./word_embeddings/encoded_features.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get array of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [x for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "datasets_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature dataset\"]\n",
    "datasets_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal dataset\"]\n",
    "attributes = [x for x,y in g.nodes(data=True) if y['tipo']==\"attribute\"]\n",
    "attributes_features = [x for x,y in g.nodes(data=True) if y['tipo']==\"feature attribute\"]\n",
    "attributes_literals = [x for x,y in g.nodes(data=True) if y['tipo']==\"literal attribute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(datasets_features))\n",
    "print(len(datasets_literals))\n",
    "print(len(attributes))\n",
    "print(len(attributes_features))\n",
    "print(len(attributes_literals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get nodes and print data from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [y for x,y in g.nodes(data=True) if y['tipo']==\"dataset\"]\n",
    "print(sample[0][\"vector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possitive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    possitive_pairs = []\n",
    "    negative_pairs = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[2] == 1:\n",
    "            possitive_pairs.append((row[0],row[1]))\n",
    "        else:\n",
    "            if randrange(1, 10) > 8:\n",
    "                negative_pairs.append((row[0],row[1]))\n",
    "    return possitive_pairs,negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching = read_dataset(\"./openML/openml_203ds_datasets_matching.csv\",keep_columns=[\"'dataset1_id'\", \"'dataset2_id'\",\"'matching_topic'\"]);\n",
    "pos,neg = get_samples(df_matching)\n",
    "print(\"Possitive samples: \"+str(len(pos)) + \" Negative samples (10%): \"+str(len(neg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    #Returns True is string is a number.\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tokens(nodes):\n",
    "    #nodes_unique = set(nodes)\n",
    "    nodes_unique = nodes\n",
    "    numerical=[]\n",
    "    nominal=[]\n",
    "    for s in nodes_unique:\n",
    "        if is_number(s):\n",
    "            numerical.append(float(s))\n",
    "        else:\n",
    "            #nominal = nominal +  list(map(lambda a : re.sub(r'[^A-Za-z0-9 \\']+','',a).lower(),(re.split('[-_;|]\\s*',s))))\n",
    "            nominal.append(re.sub(r'[^A-Za-z0-9 \\']+',' ',s).lower())\n",
    "            #nominal = nominal +  s.split(\"|\")\n",
    "    return list(set(numerical)),list(set(nominal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_from_number(n):\n",
    "    #get the embeddings for numerical values that make sense\n",
    "    out = []\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_num,vec_nom = get_tokens(lit_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(\"num: \"+ str(vec_num[i]))\n",
    "    print(\"nom: \"+ vec_nom[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_vectors(vec1,vec2):\n",
    "    return (1- cosine(vec1,vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9258200997725514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_vectors([1,2,3],[1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fasttext\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('../models/cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def fasttex_(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "\n",
    "    return torch.tensor(ft.get_sentence_vector(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def fasttex_vectors(nodes_literals):\n",
    "    nodes_vector = []\n",
    "    for literal in nodes_literals:\n",
    "        \n",
    "        if is_number(literal):\n",
    "            literal = str(literal)\n",
    "        \n",
    "        nodes_vector.append(torch.tensor(ft.get_sentence_vector(literal)))\n",
    "        \n",
    "    return nodes_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "#load model in memory\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(value):\n",
    "    if is_number(value):\n",
    "        value = str(value)\n",
    "\n",
    "    #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "    tokenized = tokenizer.encode(value, add_special_tokens=True,max_length=512)\n",
    "    input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    #result shape: (batch size, sequence length, model hidden dimension)\n",
    "    #print(last_hidden_states.shape)\n",
    "\n",
    "    #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "    return torch.mean(last_hidden_states[0],dim=0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_vectors(nodes_literals):\n",
    "    nodes_vector = []\n",
    "    for literal in nodes_literals:\n",
    "        \n",
    "        if is_number(literal):\n",
    "            literal = str(literal)\n",
    "        \n",
    "        #add special tokens at the begining and end, and takes until 512 tokens max \n",
    "        tokenized = tokenizer.encode(literal, add_special_tokens=True,max_length=512)\n",
    "        input_ids = torch.tensor(tokenized).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        #result shape: (batch size, sequence length, model hidden dimension)\n",
    "        #print(last_hidden_states.shape)\n",
    "        \n",
    "        #make the mean of the vectors to have 1 vector for the whole sentence and store result\n",
    "        nodes_vector.append(torch.mean(last_hidden_states[0],dim=0).detach()) \n",
    "    return nodes_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(data, model):\n",
    "    if model==\"fasttext\":\n",
    "        return fasttex_(data)\n",
    "    if model==\"bert\":\n",
    "        return bert(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings of words and calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "s1= \"University degree\"\n",
    "s2=\"Bachelor\"\n",
    "vectors_b = bert_vectors([s1,s2])\n",
    "vectors_ft = fasttex_vectors([s1,s2])\n",
    "print(\"Fasttext dif: \" + str(cosine_vectors(vectors_ft[0],vectors_ft[1])))\n",
    "#print(\"Bert dif: \" + str(cosine_vectors(vectors_b[0],vectors_b[1])))\n",
    "print(\"Fasttext rep\")\n",
    "print(vectors_ft[0])\n",
    "print(torch.tensor(vectors_ft[0]))\n",
    "print(\"BERT rep\")\n",
    "print(vectors_b[0])\n",
    "print(torch.tensor(vectors_b[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "rep = '%.4E' % Decimal(88799777378)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectors, columns=[\"colummn\"])\n",
    "df.to_csv('lit_vectors.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
