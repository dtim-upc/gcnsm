{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read graph of metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g_x = nx.read_gpickle(\"encoded_fasttext.gpickle\")\n",
    "#g = nx.read_gpickle(\"encoded_features.gpickle\")\n",
    "#g = nx.read_gpickle(\"siimple.gpickle\")\n",
    "order = 0\n",
    "for x,n in sorted(g_x.nodes(data=True)):\n",
    "    t = n['tipo']\n",
    "    if t == \"dataset\":\n",
    "        n['tipo'] = 0\n",
    "    if t == \"feature dataset\":\n",
    "        n['tipo'] = 1\n",
    "    if t == \"literal dataset\":\n",
    "        n['tipo'] = 2\n",
    "    if t == \"attribute\":\n",
    "        n['tipo'] = 3\n",
    "    if t == \"feature attribute\":\n",
    "        n['tipo'] = 4\n",
    "    if t == \"literal attribute\":\n",
    "        n['tipo'] = 5  \n",
    "    n['order']=order\n",
    "    order+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [x for x,y in g_x.nodes(data=True) if y['tipo']==0]\n",
    "order = [y['order'] for x,y in g_x.nodes(data=True) if y['tipo']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_order = dict(zip(datasets,order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_order['DS_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 60% train, 20% val and 20% test from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_dataset(path,drop_columns=None,keep_columns=None):\n",
    "    #get rid of useless columns\n",
    "    csv_data = pd.read_csv(path)\n",
    "    \n",
    "    if keep_columns != None:\n",
    "        #keep only these columns\n",
    "        return csv_data.filter(items=keep_columns)\n",
    "    \n",
    "    if drop_columns!= None:\n",
    "        #drop these and keep the rest\n",
    "        return csv_data.drop(drop_columns, axis=1)\n",
    "    \n",
    "    #finally, didn't drop or filter any column\n",
    "    return csv_data     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "def get_samples(df):\n",
    "    train_mask = []\n",
    "    val_mask = []\n",
    "    test_mask = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[2] == 1:\n",
    "            if randrange(0, 10) <= 7:\n",
    "                train_mask.append([row[0],row[1],row[2]])\n",
    "            else:\n",
    "                test_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "#                 if index % 2 == 0:\n",
    "#                     val_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "#                 else:\n",
    "#                     test_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "        else:\n",
    "            if randrange(0, 10) > 8:\n",
    "#                 if randrange(0, 30) <= 21:\n",
    "                if randrange(0, 30) <= 24:\n",
    "                    train_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "                else:\n",
    "                    test_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "#                     if index % 2 == 0:\n",
    "#                         val_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "#                     else:\n",
    "#                         test_mask.append([map_order['DS_'+str(row[0])],map_order['DS_'+str(row[1])],row[2]])\n",
    "    return train_mask,test_mask\n",
    "#     return train_mask,val_mask,test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2042 Test samples: 422\n"
     ]
    }
   ],
   "source": [
    "df_matching = read_dataset(\"./openml_203ds_datasets_matching.csv\",keep_columns=[\"'dataset1_id'\", \"'dataset2_id'\",\"'matching_topic'\"]);\n",
    "# train_mask,val_mask,test_mask = get_samples(df_matching)\n",
    "# print(\"Train samples: \"+str(len(train_mask)) + \" Val samples: \"+str(len(val_mask))+ \" Test samples: \"+str(len(test_mask)))\n",
    "train_mask,test_mask = get_samples(df_matching)\n",
    "print(\"Train samples: \"+str(len(train_mask)) + \" Test samples: \"+str(len(test_mask)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "#convert from networkx to graph deep library format\n",
    "g = dgl.DGLGraph()\n",
    "#gdl.from_networkx(g,['vector'])\n",
    "g.from_networkx(g_x,node_attrs=['tipo','vector'], edge_attrs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "gcn_msg = fn.copy_src(src='vector', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g,feature):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        with g.local_scope():\n",
    "            g.ndata['vector'] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata['vector']\n",
    "            return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Constrastive(nn.Module):\n",
    "#     def __init__(self, in_feats, out_feats):\n",
    "#         super(Constrastive, self).__init__()\n",
    "#         self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "#     def forward(self, g,feature):\n",
    "#         # Creating a local scope so that all the stored ndata and edata\n",
    "#         # (such as the `'h'` ndata below) are automatically popped out\n",
    "#         # when the scope exits.\n",
    "#         with g.local_scope():\n",
    "#             g.ndata['vector'] = feature\n",
    "#             g.update_all(gcn_msg, gcn_reduce)\n",
    "#             h = g.ndata['vector']\n",
    "#             return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = GCNLayer(300, 300)\n",
    "        self.layer2 = GCNLayer(300, 300)\n",
    "    \n",
    "    def forward(self, g,features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = F.relu(self.layer2(g, x))\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "# def cosine_vectors(vec1,vec2):\n",
    "#     cos = th.nn.CosineSimilarity()\n",
    "#     c1 = cos(vec1,vec2)\n",
    "#     c2 = (1- cosine(vec1,vec2))\n",
    "#     print(c1-c2)\n",
    "#     return c1\n",
    "\n",
    "# def resultSet_train(features,mask):\n",
    "#     indices = []\n",
    "#     labels = []\n",
    "#     for n in mask:\n",
    "#         resultCos = cosine(features[n[0]],features[n[1]])\n",
    "#         # index 0 is not similar, index 1 is similar\n",
    "#         result = [1 - resultCos , resultCos]\n",
    "#         indices.append(result)\n",
    "#         labels.append(n[2])\n",
    "#     return th.tensor(indices, requires_grad=True),th.tensor(labels)\n",
    "\n",
    "\n",
    "def resultSet_train(features,mask):\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    labels = []\n",
    "    for n in mask:\n",
    "        v1.append(features[n[0]])\n",
    "        v2.append(features[n[1]])\n",
    "        if n[2] == 0:\n",
    "            n[2] = -1\n",
    "        labels.append(n[2])\n",
    "    return th.stack(v1),th.stack(v2),th.tensor(labels)\n",
    "\n",
    "\n",
    "def resultSet_eval(features,mask):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    for n in mask:\n",
    "        resultCos = (1 - cosine(features[n[0]],features[n[1]]))\n",
    "        # index 0 is not similar, index 1 is similar\n",
    "        result = th.tensor([1 - resultCos , resultCos])\n",
    "        # get the index that is greater\n",
    "        out = th.max(result)\n",
    "        indices.append(out)\n",
    "        labels.append(n[2])\n",
    "    return th.tensor(indices, requires_grad=True),th.tensor(labels)\n",
    "\n",
    "def evaluate(model, g, features, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        embeddings = model(g, features)\n",
    "        indices , labels = resultSet_eval(embeddings,mask)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "# net.train()\n",
    "# embeddings = net(g, g.ndata['vector'])\n",
    "# indices,labels = resultSet_train(embeddings,train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e1,e2,labels = resultSet_train(embeddings,train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_tensor = th.cat(e1,dim=0)\n",
    "my_tensor = th.stack(e1)\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 0.7698 | Test Acc 0.0024 | Time(s) 1002.9572\n",
      "Epoch 00001 | Loss 0.7417 | Test Acc 0.0024 | Time(s) 995.8885\n",
      "Epoch 00002 | Loss 0.6072 | Test Acc 0.0024 | Time(s) 994.3604\n",
      "Epoch 00003 | Loss 0.5148 | Test Acc 0.0047 | Time(s) 993.3139\n",
      "Epoch 00004 | Loss 0.4534 | Test Acc 0.0569 | Time(s) 994.2525\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "# net = Net()\n",
    "# optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "# dur = []\n",
    "# loss_func = nn.CosineEmbeddingLoss()\n",
    "for epoch in range(15):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    net.train()\n",
    "    embeddings = net(g, g.ndata['vector'])\n",
    "    v1,v2,labels = resultSet_train(embeddings,train_mask)\n",
    "    loss = loss_func(v1,v2, labels)\n",
    "    optimizer.zero_grad()\n",
    "    #loss.backward(retain_graph=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    \n",
    "    #acc = evaluate(net, g, embeddings.detach(), test_mask)\n",
    "    acc = evaluate(net, g, g.ndata['vector'], test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th.tensor(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# net = Net()\n",
    "# #g, features, labels, train_mask, test_mask\n",
    "\n",
    "# optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "# dur = []\n",
    "# for epoch in range(20):\n",
    "#     if epoch >=3:\n",
    "#         t0 = time.time()\n",
    "\n",
    "#     net.train()\n",
    "#     embeddings = net(g, g.ndata['vector'])\n",
    "#     indices,labels = resultSet_train(embeddings.detach(),train_mask)\n",
    "#     logp = F.logsigmoid(indices)\n",
    "#     loss = F.nll_loss(logp, labels)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     if epoch >=3:\n",
    "#         dur.append(time.time() - t0)\n",
    "    \n",
    "#     acc = evaluate(net, g, embeddings.detach(), test_mask)\n",
    "#     print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "#             epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from a specific node\n",
    "g.nodes[0].data\n",
    "#get data from nodes\n",
    "g.ndata\n",
    "#another way of accessing data from a node\n",
    "g.ndata['tipo'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.has_edge_between(374,17619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in g.nodes:\n",
    "    print (n.data['tipo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
