{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 80% train, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_mask = pd.read_csv(\"./datasets/train2.csv\").to_numpy()\n",
    "test_mask = pd.read_csv(\"./datasets/test2.csv\").to_numpy()\n",
    "# test_negative_mask = pd.read_csv(\"./datasets/test_only_negative2.csv\").to_numpy()\n",
    "# print(\"Train samples: \"+str(len(train_mask)) + \" Test samples: \"+str(len(test_mask)) + \" Test negative samples: \"+str(len(test_negative_mask)))\n",
    "print(\"Train samples: \"+str(len(train_mask)) + \" Test samples: \"+str(len(test_mask)))\n",
    "# nodes_train = np.unique(np.concatenate((train_mask[:,0],train_mask[:,1]))) \n",
    "# nodes_test = np.unique(np.concatenate((test_mask[:,0],test_mask[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read graph of metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g_x = nx.read_gpickle(\"./word_embeddings/encoded_fasttext.gpickle\")\n",
    "#g_x = nx.read_gpickle(\"./word_embeddings/encoded_bert.gpickle\")\n",
    "#g_x = nx.read_gpickle(\"./word_embeddings/siimple.gpickle\")\n",
    "order = 0\n",
    "for x,n in sorted(g_x.nodes(data=True)):\n",
    "    t = n['tipo']\n",
    "    if t == \"dataset\":\n",
    "        n['tipo'] = 0\n",
    "    if t == \"feature dataset\":\n",
    "        n['tipo'] = 1\n",
    "    if t == \"literal dataset\":\n",
    "        n['tipo'] = 2\n",
    "    if t == \"attribute\":\n",
    "        n['tipo'] = 3\n",
    "    if t == \"feature attribute\":\n",
    "        n['tipo'] = 4\n",
    "    if t == \"literal attribute\":\n",
    "        n['tipo'] = 5  \n",
    "    n['order']=order\n",
    "    order+=1\n",
    "    \n",
    "datasets = [x for (x,y) in g_x.nodes(data=True) if y['tipo']==0]\n",
    "order = [y['order'] for x,y in g_x.nodes(data=True) if y['tipo']==0]\n",
    "map_order = dict(zip(datasets,order))\n",
    "map_reverse_order = dict(zip(order,datasets))\n",
    "map_order['DS_1']\n",
    "\n",
    "for mask in train_mask:\n",
    "    mask[0] = map_order[\"DS_\"+str(mask[0])]\n",
    "    mask[1] = map_order[\"DS_\"+str(mask[1])]\n",
    "for mask in test_mask:\n",
    "    mask[0] = map_order[\"DS_\"+str(mask[0])]\n",
    "    mask[1] = map_order[\"DS_\"+str(mask[1])]\n",
    "# for mask in test_negative_mask:\n",
    "#     mask[0] = map_order[\"DS_\"+str(mask[0])]\n",
    "#     mask[1] = map_order[\"DS_\"+str(mask[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "#convert from networkx to graph deep library format\n",
    "g = dgl.DGLGraph()\n",
    "#gdl.from_networkx(g,['vector'])\n",
    "g.from_networkx(g_x,node_attrs=['tipo','vector'], edge_attrs=None)\n",
    "g_x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultSet_train(features,mask):\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    labels = []\n",
    "    for n in mask:\n",
    "        v1.append(features[n[0]])\n",
    "        v2.append(features[n[1]])\n",
    "        if n[2] == 0:\n",
    "            n[2] = -1\n",
    "        labels.append(n[2])\n",
    "    return th.stack(v1),th.stack(v2),th.tensor(labels)\n",
    "\n",
    "def resultSet_train_softmax(features,mask,labels):\n",
    "    v1 = []\n",
    "    labels_out = [] \n",
    "    loaded = []\n",
    "    for n in mask:\n",
    "        if n[0] not in loaded:\n",
    "            loaded.append(n[0])\n",
    "            v1.append(features[n[0]])\n",
    "            labels_out.append(labels[n[0]])\n",
    "        if n[1] not in loaded:\n",
    "            loaded.append(n[1])\n",
    "            v1.append(features[n[1]])\n",
    "            labels_out.append(labels[n[0]])\n",
    "    return th.stack(v1),th.tensor(labels_out)\n",
    "\n",
    "def normalization(vector):\n",
    "    return (vector / th.norm(vector))\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "def resultSet_eval(features,mask,loss,sim=0.7):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    \n",
    "    if loss == \"ContrastiveLoss\" or loss == \"Euclidean\":\n",
    "        pdist = th.nn.PairwiseDistance(p=2)        \n",
    "        v1 = features[mask[:,0]]\n",
    "        v2 = features[mask[:,1]]\n",
    "        result = pdist(v1,v2)\n",
    "        for r in result:\n",
    "            if r.item() <= sim:\n",
    "                indices.append(1.0)\n",
    "            else:\n",
    "                indices.append(0.0)\n",
    "                \n",
    "        return th.tensor(indices),th.tensor(mask[:,2])\n",
    "    \n",
    "    if loss == \"CosineEmbeddingLoss\":\n",
    "        for n in mask:\n",
    "\n",
    "            cos = th.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "            result = cos(features[n[0]],features[n[1]])\n",
    "\n",
    "            if result.item() >= sim:\n",
    "                out = th.tensor(1)\n",
    "            else:\n",
    "                out = th.tensor(0)\n",
    "\n",
    "            indices.append(out)\n",
    "            labels.append(n[2])\n",
    "        return th.tensor(indices),th.tensor(labels)\n",
    "\n",
    "def evaluate(model, g, features, mask,loss,eval_sim):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        embeddings = model(g, features)\n",
    "        indices , labels = resultSet_eval(embeddings,mask,loss,eval_sim)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(sim):\n",
    "#     total_data = np.concatenate((test_mask,train_mask))\n",
    "#     neg_data = np.array([x for x in total_data if x[2]==0])\n",
    "#     pos_data = np.array([x for x in total_data if x[2]==1])\n",
    "#     print(len(neg_data))\n",
    "#     print(len(pos_data))\n",
    "#     acc_pos = evaluate(net, g, g.ndata['vector'], pos_data,\"CosineEmbeddingLoss\",sim)\n",
    "#     acc_neg = evaluate(net, g, g.ndata['vector'], neg_data,\"CosineEmbeddingLoss\",sim)\n",
    "#     return (\"Accuracy with possitive: \"+ str(acc_pos) + \" and negatives: \" + str(acc_neg))\n",
    "    return evaluate(net, g, g.ndata['vector'], train_mask,\"CosineEmbeddingLoss\",sim)\n",
    "# test_model(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_similarity(training):\n",
    "    total_data = np.concatenate((test_mask,train_mask))\n",
    "    pos_data = np.array([x for x in total_data if x[2]==1])\n",
    "    training.net.eval()\n",
    "    embeddings = training.net(g, g.ndata['vector'])\n",
    "    ds_embeddings = embeddings[list(map_reverse_order.keys())]\n",
    "    #print(ds_embeddings.size())\n",
    "    pdist = th.nn.PairwiseDistance(p=2)        \n",
    "    thecos = th.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    sum_accuracy = 0\n",
    "    no_pairs = 0\n",
    "    check_vector1 = []\n",
    "    check_vector2 = []\n",
    "    check_vector = []\n",
    "    for i in map_reverse_order:\n",
    "        candidate = embeddings[i]\n",
    "        if training.loss_name == \"ContrastiveLoss\":\n",
    "            result = pdist(candidate,ds_embeddings)\n",
    "            closest_indices = th.topk(result, 4, largest=False).indices[1:]\n",
    "            closest = np.array([list(map_reverse_order.keys())[x] for x in closest_indices])\n",
    "            check_vector1 = np.array([x[1] for x in pos_data if x[0]==i and x[1] not in check_vector])\n",
    "            check_vector2 = np.array([x[0] for x in pos_data if x[1]==i and x[0] not in check_vector])\n",
    "            check_vector = np.concatenate((check_vector1,check_vector2),axis=0)\n",
    "            closest_checked = np.array([x for x in check_vector if x in closest])\n",
    "            if len(check_vector) > 0:\n",
    "                accuracy = len(closest_checked) / min(len(closest),len(check_vector))\n",
    "                sum_accuracy = sum_accuracy + accuracy\n",
    "#                 print(\"Accuracy for: \"+ str(i) + \" = \" + str(accuracy))\n",
    "            else: \n",
    "                no_pairs +=1\n",
    "#                 print(\"No pairs, best distance is: \"+ str(result[closest_indices[0]]))\n",
    "        if training.loss_name == \"CosineEmbeddingLoss\":\n",
    "            result = th.clamp(thecos(candidate.reshape(1,len(candidate)),ds_embeddings),min=0)\n",
    "            closest_indices = th.topk(result, 20, largest=True).indices[1:]\n",
    "            closest = np.array([list(map_reverse_order.keys())[x] for x in closest_indices])\n",
    "            check_vector1 = np.array([x[1] for x in pos_data if x[0]==i and x[1] not in check_vector])\n",
    "            check_vector2 = np.array([x[0] for x in pos_data if x[1]==i and x[0] not in check_vector])\n",
    "            check_vector = np.concatenate((check_vector1,check_vector2),axis=0)\n",
    "            closest_checked = np.array([x for x in check_vector if x in closest])\n",
    "            if len(check_vector) > 0:\n",
    "                accuracy = len(closest_checked) / min(len(closest),len(check_vector))\n",
    "                sum_accuracy = sum_accuracy + accuracy\n",
    "#                 print(\"Accuracy for: \"+ str(i) + \" = \" + str(accuracy))\n",
    "            else: \n",
    "                no_pairs +=1\n",
    "#                 print(\"No pairs, best distance is: \"+ str(result[closest_indices[0]]))\n",
    "    return sum_accuracy / (len(map_reverse_order) - no_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   test_similarity(\"CosineEmbeddingLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### NN architectures: \n",
    "0: 'Bert_768',  <br>\n",
    "1: 'Fasttext_150', <br>\n",
    "2: 'Fasttext_150_150_100',  <br>\n",
    "3: 'Fasttext_300' <br>\n",
    "### Loss functions: \n",
    "0: 'ContrastiveLoss', <br>\n",
    "1: 'CosineEmbeddingLoss', <br>\n",
    "2: 'Euclidean' <br>\n",
    "#### Example to define architecture and loss\n",
    "import step3_gcn_nn_concatenate as gcn_nn <br>\n",
    "import step3_gcn_loss as gcn_loss <br>\n",
    "print(gcn_nn.get_options()) #list of options<br>\n",
    "print(gcn_loss.get_options()) #list of options<br>\n",
    "print(gcn_nn.get_instance(option=0,name=None)) #or gcn_nn.get_instance(option=None,name=\"Bert_768\") <br> print(gcn_loss.get_instance(0,margin=0.5,reduction=\"sum\")) #or gcn_loss.get_instance(0,margin=0.5,reduction=\"sum\") <br>\n",
    "\n",
    "### Load training class to save/load/train experiments:\n",
    "import step3_gcn_train as gcn_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "def train(training,iterations):\n",
    "    dur = []\n",
    "    ## training.splits indicates number of sets to split, not batch size!\n",
    "    train_batch = np.array_split(train_mask,training.batch_splits)\n",
    "    #th.set_num_threads(2)\n",
    "    for epoch in range(iterations):\n",
    "        training.net.train()\n",
    "        t0 = time.time()\n",
    "        for split in train_batch:\n",
    "            embeddings = training.net(g, g.ndata['vector'])\n",
    "            v1,v2,labels = resultSet_train(embeddings,split)\n",
    "            loss = training.loss(v1,v2, labels)\n",
    "            training.optimizer.zero_grad()\n",
    "            #loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            training.optimizer.step()\n",
    "\n",
    "        #runtime\n",
    "        t = time.time() - t0\n",
    "        dur.append(t)\n",
    "        training.runtime_seconds+=t\n",
    "        \n",
    "        #accuracy\n",
    "        acc = evaluate(training.net, g, g.ndata['vector'], test_mask,training.loss_name, 0.90)\n",
    "        acc2 = test_similarity(training)\n",
    "        \n",
    "        #create log\n",
    "        output = {}\n",
    "        output['epoch'] = training.epochs_run\n",
    "        output['loss'] = float('%.5f'% (loss.item()))\n",
    "        output['acc'] = float('%.5f'% (acc))\n",
    "        output['acc2'] = float('%.5f'% (acc2))\n",
    "        output['time_epoch'] = float('%.5f'% (np.mean(dur)))\n",
    "        output['time_total'] = float('%.5f'% (training.runtime_seconds))\n",
    "        training.log.append(output)\n",
    "        training.epochs_run+=1\n",
    "        print(str(output))\n",
    "        \n",
    "    #write results and save model to files\n",
    "    training.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import step3_gcn_nn_concatenate as gcn_nn\n",
    "import step3_gcn_loss as gcn_loss\n",
    "import step3_gcn_training as gcn_training\n",
    "\n",
    "training = gcn_training.Training()\n",
    "\n",
    "#load model from path\n",
    "training.load_state(path=\"./models/net_name:Fasttext_150|batch_splits:40.0000|lr:0.0010|loss_name:CosineEmbeddingLoss|loss_parameters:0.0+mean.pt\")\n",
    "\n",
    "#train new model and specify parameters\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(1),\n",
    "#             batch_splits=40,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.0+mean\")\n",
    "\n",
    "train(training,iterations=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
