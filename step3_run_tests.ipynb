{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThese are the default values\\n\\nneg_sample = 2\\nstrategy = \"random\"\\ncreate_new_split = False #assumes splitted files exists already\\nword_embedding_encoding = \"FASTTEXT\"\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "choose integer number of ratio negative/positive to sample (0 will use all negative pairs)\n",
    "\"\"\"\n",
    "neg_sample = 2\n",
    "\"\"\"\n",
    "Choose one split trategy [\"isolation\",\"random\"] : \n",
    "- random will randomly spread positive node pairs in 80-20 fashion\n",
    "- isolation will isolate 1 node from some topics in test (none pair in train will see these nodes).\n",
    "The positive pairs will be splitted almost in 80-20%, like in the random case.\n",
    "\"\"\"\n",
    "strategy = \"random\"\n",
    "\"\"\"\n",
    "Choose to use the selected strategy to create a new split \n",
    "or reuse a previously created one (useful to repeat exact same experiment)\n",
    "\"\"\"\n",
    "create_new_split = False\n",
    "\n",
    "\"\"\"\n",
    "You can choose to use one of [\"FASTTEXT\",\"BERT\"] as initial word_embedding encoding for the nodes in the datasets\n",
    "\"\"\"\n",
    "word_embedding_encoding = \"FASTTEXT\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "These are the default values\n",
    "\n",
    "neg_sample = 2\n",
    "strategy = \"random\"\n",
    "create_new_split = False #assumes splitted files exists already\n",
    "word_embedding_encoding = \"FASTTEXT\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter error in parameter setup neg_sample, default values will be used\n",
      "Values to load\n",
      "neg_sample= 2\n",
      "strategy= random\n",
      "create_new_split= False\n",
      "word_embedding_encoding= FASTTEXT\n",
      "Dataset splits loaded\n",
      "Train samples: 1736 Test samples: 218\n",
      "Train positive samples: 868 Test positive samples: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-feature graph from datasets loaded\n"
     ]
    }
   ],
   "source": [
    "from step3 import step3_gcnsm\n",
    "from step3.step3_gcnsm import confusion_matrix as confusion_matrix\n",
    "from step3.step3_gcnsm import train as train\n",
    "from step3 import step3_gcn_nn_concatenate as gcn_nn\n",
    "from step3 import step3_gcn_loss as gcn_loss\n",
    "from step3 import step3_gcn_training as gcn_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose NN architecture and loss function, then run tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and run training\n",
    "### NN architectures: \n",
    "{<br>\n",
    "    \"0\": \"Bert_300\", <br>\n",
    "    \"1\": \"Bert_300_300_200\", <br>\n",
    "    \"2\": \"Bert_768\", <br>\n",
    "    \"3\": \"Fasttext3GCN_300\" <br>\n",
    "    \"4\": \"Fasttext_150\", <br>\n",
    "    \"5\": \"Fasttext_150_150_100\", <br>\n",
    "    \"6\": \"Fasttext_300\" <br>\n",
    "}\n",
    "### Loss functions: \n",
    "{<br>\n",
    "    \"0\": \"ContrastiveLoss\", <br>\n",
    "    \"1\": \"CosineEmbeddingLoss\", <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load model from path\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/[file_name].pt\")\n",
    "# train(training,iterations=N)\n",
    "\n",
    "# #train new model and specify parameters\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(),  #_of_option for NN architecture\n",
    "#             batch_splits= ,#_of_sets(this will (give dataset / batch_splits) size of batch\n",
    "#             lr= , #learning rate for training (e.g. 1e-3 )\n",
    "#             loss_name=gcn_loss.get_option_name() #_of_option for loss ,\n",
    "#             loss_parameters=) #loss function parameters separated by '+' e.g. for cosine and contrastive \"0.0+mean\" \n",
    "# train(training,iterations=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training = gcn_training.Training()\n",
    "# # training.set_training(\n",
    "# #             net_name= gcn_nn.get_option_name(4),\n",
    "# #             batch_splits=28,\n",
    "# #             lr=1e-3,\n",
    "# #             loss_name=gcn_loss.get_option_name(0),\n",
    "# #             loss_parameters=\"0.7+mean\")\n",
    "# # train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# # training.set_training(\n",
    "# #             net_name= gcn_nn.get_option_name(4),\n",
    "# #             batch_splits=28,\n",
    "# #             lr=1e-3,\n",
    "# #             loss_name=gcn_loss.get_option_name(0),\n",
    "# #             loss_parameters=\"0.5+mean\")\n",
    "# training.load_state(path=\"./models/random/2/best/net_name:Fasttext_150|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.5+mean.pt\")\n",
    "# train(training,iterations=6)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(4),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"0.3+mean\")\n",
    "# train(training,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(4),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.7+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(4),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.5+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(4),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.3+mean\")\n",
    "# train(training,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"0.7+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"0.5+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"0.3+mean\")\n",
    "# train(training,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.7+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.5+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(1),\n",
    "#             loss_parameters=\"0.3+mean\")\n",
    "# train(training,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(4),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"0.9+mean\")\n",
    "# train(training,iterations=50)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.set_training(\n",
    "#             net_name= gcn_nn.get_option_name(6),\n",
    "#             batch_splits=28,\n",
    "#             lr=1e-3,\n",
    "#             loss_name=gcn_loss.get_option_name(0),\n",
    "#             loss_parameters=\"0.9+mean\")\n",
    "# train(training,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## run until epoch 70 with bigger batch size (split = 14)\n",
    "# ##random until epoch 70 with higher batch size\n",
    "\n",
    "# ##fasttext300\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/random/2/until_epoch_50/net_name:Fasttext_300|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.5+mean.pt\")\n",
    "# training.batch_splits = 14\n",
    "# train(training,iterations=20)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/random/2/until_epoch_50/net_name:Fasttext_300|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.7+mean.pt\")\n",
    "# training.batch_splits = 14\n",
    "# train(training,iterations=20)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/random/2/until_epoch_50/net_name:Fasttext_300|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.9+mean.pt\")\n",
    "# training.batch_splits = 14\n",
    "# train(training,iterations=20)\n",
    "\n",
    "\n",
    "# ##fasttext150\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/random/2/until_epoch_50/net_name:Fasttext_150|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.5+mean.pt\")\n",
    "# training.batch_splits = 14\n",
    "# train(training,iterations=20)\n",
    "\n",
    "# training = gcn_training.Training()\n",
    "# training.load_state(path=\"./models/random/2/until_epoch_50/net_name:Fasttext_150|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.7+mean.pt\")\n",
    "# training.batch_splits = 14\n",
    "# train(training,iterations=20)\n",
    "\n",
    "training = gcn_training.Training()\n",
    "training.load_state(path=\"./models/random/2/until_epoch_50/net_name:Fasttext_150|batch_splits:28.0000|lr:0.0010|loss_name:ContrastiveLoss|loss_parameters:0.9+mean.pt\")\n",
    "training.batch_splits = 14\n",
    "train(training,iterations=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print confusion matrix and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(training.net, g, g.ndata['vector'], test_mask,training.loss_name,0.2):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
